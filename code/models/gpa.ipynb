{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting GPA with Deep Learning\n",
    "This notebook reproduces the models and all figures and tables contained in my paper on the Fragile Familes Challenge.\n",
    "\n",
    "***Please note the following if you intend to run this notebook***\n",
    "\n",
    "- To the best of my knowledge this notebook will reproduce all the results accurately but due to stochastic nature of many of the processes used the results may differ. Where possible I have created static copies of objects that can be loaded directly. Some of these are too large to store on Github, for example the pickled versions of the final 5 classifiers. Please email me directly if you would like copies of these.\n",
    "\n",
    "- This notebook is contains process that are computationally intensive and take some time to run. As is it will take at least 24 hours to run on a top spec laptop computer. I have noted the cells that take most time to run. If possible you may consider editing the notebook where appropriate to run processes in paraellel or using a GPU, although this may impact reproducibility.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loading packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up to ensure reproducibility following https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(67891)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "random.seed(54321)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "tf.set_random_seed(56789)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "seed = 13579 # used below to seed sklearn functions\n",
    "\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.callbacks import EarlyStopping, CSVLogger, History\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the files\n",
    "\n",
    "***Note: These data cannot be provided on Github and I will delete my copies in accordance with the FFC agreement. If you would like copies of the data to replicate these analyses please consult the Fragile Families and Child Wellbeing Survey [website](https://fragilefamilies.princeton.edu/documentation).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('../../../FFChallenge_v2/train.csv',low_memory=False, index_col='challengeID')\n",
    "predictions=pd.read_csv('../../../FFChallenge_v2/prediction.csv',low_memory=False, index_col='challengeID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a version of the data with missing values imputed (`full_imputed.csv`) the script `clean_files.py` must first be run. If necessary it can be executed by uncommenting (deleting the #) and running the line below. ***This script will take approximately 30 minutes to run. It only needs to be run once.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! PYTHONHASHSEED=0 python3 ../preprocess/clean_files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/full_imputed.csv') # load imputed data output after running the clean_files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index = data['challengeID']\n",
    "del data['challengeID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the outcomes from the imputed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[['gpa','grit','materialHardship','eviction','layoff','jobTraining']]\n",
    "X = data\n",
    "for c in X.columns:\n",
    "    if c in list(y.columns):\n",
    "        del X[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "\n",
    "Before modelling the data there are two types of transformations that I use to optimize them for the neural network.\n",
    "\n",
    "Categorical variables are transformed using one-hot encoding. Continuous variables are also normalized to have a mean of zero.\n",
    "\n",
    "To identify which columns belong to which group I use same heuristic as in the imputation script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "cat_cols = []\n",
    "non_cat_cols = []\n",
    "for i, c in enumerate(X.columns):\n",
    "    is_categorical = False\n",
    "    vals = set(list(X[c]))\n",
    "    vals = {x for x in vals if x==x} # Removes nans, otherwise treated as unique\n",
    "    if X[c].dtype == 'float64': # if float and low num distinct then treat as cat\n",
    "        if len(vals) <= 20:\n",
    "            is_categorical = True\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        is_categorical = True\n",
    "    \n",
    "    # Now append to relevant list of columns\n",
    "    if is_categorical:\n",
    "        cat_cols.append(c)\n",
    "        \n",
    "    else:\n",
    "        non_cat_cols.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dummies = pd.get_dummies(X, columns=cat_cols)\n",
    "# Note that sklearn also has one-hot encoding but doesn't relabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = StandardScaler()\n",
    "for c in non_cat_cols:\n",
    "    normed = normalizer.fit_transform(X_dummies[c].values.reshape(-1,1))\n",
    "    X_dummies[c] = normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_dummies # rename X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now splitting the X and y matrices to separate cases in the training set and the prediction set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training=X.loc[X.index.isin(train.index)]\n",
    "X_pred=X.loc[~X.index.isin(train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_training=y.loc[y.index.isin(train.index)]\n",
    "y_pred=y.loc[~y.index.isin(train.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly splitting the data into training and test sets, where 20% of data is held out for validation and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_training, y_training.gpa, test_size=0.20, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing these files for later\n",
    "X_test.to_csv('../../data/X_test.csv')\n",
    "pd.Series(cat_cols).to_csv('../../data/cat_cols.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a function that can be used to return Keras models with different parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(activation_function, num_hidden_layers, hidden_layer_size):\n",
    "    '''\n",
    "    A function to create a Keras sequential model based on input parameters.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "        activation_function: str\n",
    "            Activation function to be used in model.\n",
    "        \n",
    "        num_hidden_layers: int\n",
    "            Number of hidden layers in model\n",
    "        \n",
    "        hidden_layer_size: int\n",
    "            Number of units/neurons in each hidden layer\n",
    "            \n",
    "    Returns\n",
    "    ---------\n",
    "    \n",
    "    Keras Sequential model object\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    \n",
    "    # Single layer model\n",
    "    if num_hidden_layers == 0: # then just specify a single layer, 1 is size of output\n",
    "        model.add(Dense(1, \n",
    "                        input_dim=X_train.shape[1], \n",
    "                        activation=activation_function,\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer=glorot_uniform(seed=seed)\n",
    "                      ))\n",
    "        \n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "    # Specify initial layer with a hidden layer\n",
    "    if num_hidden_layers >= 1: \n",
    "        model.add(Dense(hidden_layer_size, \n",
    "                        input_dim=X_train.shape[1], \n",
    "                        activation=activation_function,\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer=glorot_uniform(seed=seed)\n",
    "                       ))\n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "    # Now add additional hidden layers\n",
    "    for i in range(0,num_hidden_layers-1):\n",
    "        model.add(Dense(hidden_layer_size, \n",
    "                        activation=activation_function, \n",
    "                        use_bias=True,\n",
    "                        kernel_initializer=glorot_uniform(seed=seed)\n",
    "                       ))\n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "    if num_hidden_layers > 0:       \n",
    "        model.add(Dense(1)) # Final output layer, don't add if no hidden layers\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I take the model object and use it to initialize a classifier using the scikit-learn Keras wrapped object `KerasRegressor`.\n",
    "\n",
    "I then define the parameter space to search over and pass both to a `GridSearchCV` object. \n",
    "\n",
    "Once the `fit` method is called the grid search will begin and a model will fit for every parameter combination and fold (40 x 5). \n",
    "\n",
    "***Note: This will take 12 hours or more to complete***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "classifier = KerasRegressor(make_model, batch_size=32, epochs=200)\n",
    "\n",
    "params = [{'num_hidden_layers': [0],\n",
    "          'hidden_layer_size': [0],\n",
    "          'activation_function': ['linear', 'sigmoid', 'relu', 'tanh']},\n",
    "          {'num_hidden_layers': [1,2,3],\n",
    "          'hidden_layer_size': [64, 128, 256],\n",
    "          'activation_function': ['linear', 'sigmoid', 'relu', 'tanh']}]\n",
    "\n",
    "grid = GridSearchCV(classifier,\n",
    "                         param_grid=params,\n",
    "                         scoring='neg_mean_squared_error', #sklearn optimizing by maximizing negative MSE\n",
    "                         n_jobs=1,\n",
    "                         verbose=2,\n",
    "                         cv=KFold(n_splits=5, shuffle=True, random_state=42),# Number of folds for CV\n",
    "                         return_train_score=True,\n",
    "                         error_score='raise'\n",
    "                   )\n",
    "\n",
    "grid.fit(np.array(X_train), np.array(y_train),\n",
    "        **{'callbacks': [EarlyStopping(monitor='val_loss', \n",
    "                                                                 min_delta=0.001, \n",
    "                                                                 patience=25, \n",
    "                                                                 mode='min',\n",
    "                                                                 verbose=2)],\n",
    "                                    'validation_data': (np.array(X_test), np.array(y_test))\n",
    "                                    })\n",
    "\n",
    "print('The parameters of the best model are: ')\n",
    "print(grid.best_params_)\n",
    "\n",
    "best_model = grid.best_estimator_ #scikit-wrapped best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores can the be extracted to view performance of every model on every fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(['rank_test_score'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing models (based on the grid-search) can then be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = results.groupby(['param_num_hidden_layers','param_hidden_layer_size','param_activation_function'])\n",
    "grouped = grouped.mean()\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.to_csv('../../output/model_params_and_results.csv') # Save csv so it can be used as a table in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find the top 5 best performing models and look more closely at their performance. \n",
    "\n",
    "Note that they all have very similar scores, all of which are far better than we would expect (none of the leaderboard scores are better than 0.38). This suggests that there is some overfitting in these models, despite the use of dropout, cross-validation, and early stopping using validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = grouped.sort_values(['rank_test_score'],ascending=True).head(5)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_models = []\n",
    "for _, r in best.iterrows():\n",
    "    p = {'num_hidden_layers': [_[0]],\n",
    "          'hidden_layer_size': [_[1]],\n",
    "          'activation_function': [_[2]]}\n",
    "    final_models.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to retrain the 5 best models and assess the out-of-sample performance of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can use the same gridsearch with a single set of parameters to iterate over the models and get predictions. The only changes I'm making are the inclusion of additional callback parameters to store the results and a slight decrease in the `patience` parameter to help prevent overfitting.\n",
    "\n",
    "This time I also store each estimator, its predictions, and its history in a dictionary.\n",
    "\n",
    "***Note: It takes at least 2 hours to run these models.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seeds again to ensure reproducibility\n",
    "np.random.seed(67891)\n",
    "random.seed(54321)\n",
    "tf.set_random_seed(56789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_info = {}\n",
    "for i, p in enumerate(final_models):\n",
    "    history = History() # History callback\n",
    "    model_name = 'model_'+str(i+1)\n",
    "    classifier = KerasRegressor(make_model, batch_size=32, epochs=200)\n",
    "    grid = GridSearchCV(classifier,\n",
    "                             param_grid=p,\n",
    "                             scoring='neg_mean_squared_error', #sklearn optimizing by maximizing negative MSE\n",
    "                             n_jobs=1,\n",
    "                             verbose=2,\n",
    "                             cv=KFold(n_splits=5, shuffle=True, random_state=42),# Number of folds for CV\n",
    "                             return_train_score=True\n",
    "                       )\n",
    "\n",
    "    grid.fit(np.array(X_train), np.array(y_train),\n",
    "            **{'callbacks': [EarlyStopping(monitor='val_loss', \n",
    "                                                                     min_delta=0.001, \n",
    "                                                                     patience=20,\n",
    "                                                                     mode='min',\n",
    "                                                                     verbose=2),\n",
    "                                                      CSVLogger('../../output/logs/'+model_name+'.csv', separator=',', append=True),\n",
    "                                                      history], # Added a history callback\n",
    "                                        'validation_data': (np.array(X_test), np.array(y_test))\n",
    "                                        })\n",
    "    y_hats = grid.predict(np.array(X_test))\n",
    "    print(\"Out-of-sample MSE: \", mean_squared_error(y_test, y_hats))\n",
    "    y_hats_full = grid.predict(np.array(X))\n",
    "    model_info[model_name] = {'grid_obj': grid,\n",
    "                              'keras_model': grid.best_estimator_.model,\n",
    "                              'preds': y_hats_full,\n",
    "                              'history': history.history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = {k:v['history'] for k,v in model_info.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(histories, open('../../output/model_histories.p','wb')) # Storing for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess how the different models performed we can plot their performance on the training and test set at each epoch in the learning process. Since the first couple of epochs generally have a very high loss (thus increasing the size of the y-axis and obscuring the future epochs) I present the learn from the results from the second epoch onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, k in enumerate(list(model_info.keys())): \n",
    "    print(final_models[i])\n",
    "    plt.plot(model_info[k]['history']['loss'][2:])\n",
    "    plt.plot(model_info[k]['history']['val_loss'][2:])\n",
    "    plt.title('Mean squared error over training')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and third models both show evidence of overfitting, as training performance continued to improve as validation performance plateaued (as seen when the blue line goes below the orange line). This suggests that these models will perform worse on the out-of-sample data.\n",
    "\n",
    "Now I store and upload the predictions of each model to obtain the results on the true held out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, k in enumerate(list(model_info.keys())):\n",
    "    predictions['gpa'] = model_info[k]['preds']\n",
    "    name = '../../output/final_predictions_model_'+str(i)+'.csv'\n",
    "    predictions.to_csv(name)\n",
    "    # csvs were then uploaded to the challenge website: https://codalab.fragilefamilieschallenge.org/#participate-submit_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to compare to the predicted values for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing performance on validation set\n",
    "for k,v in model_info.items():\n",
    "    print(mean_squared_error(y_test, v['grid_obj'].predict(np.array(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all five model objects\n",
    "for k,v in model_info.items():\n",
    "    v['keras_model'].save('../../output/models/'+k+'.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
