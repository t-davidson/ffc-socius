{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting GPA with Deep Learning\n",
    "This notebook reproduces the models and all figures and tables contained in my paper on the Fragile Familes Challenge.\n",
    "\n",
    "***Please note the following if you intend to run this notebook***\n",
    "\n",
    "- To the best of my knowledge this notebook will reproduce all the results accurately but due to stochastic nature of many of the processes used the results may differ. Where possible I have created static copies of objects that can be loaded directly. Some of these are too large to store on Github, for example the pickled versions of the final 5 classifiers. Please email me directly if you would like copies of these.\n",
    "\n",
    "- This notebook is contains process that are computationally intensive and take some time to run. As is it will take at least 24 hours to run on a top spec laptop computer. I have noted the cells that take most time to run. If possible you may consider editing the notebook where appropriate to run processes in paraellel or using a GPU, although this may impact reproducibility.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loading packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Set up to ensure reproducibility following https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(67891)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "random.seed(54321)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "tf.set_random_seed(56789)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "seed = 13579 # used below to seed sklearn functions\n",
    "\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.callbacks import EarlyStopping, CSVLogger, History\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the files\n",
    "\n",
    "***Note: These data cannot be provided on Github and I will delete my copies in accordance with the FFC agreement. If you would like copies of the data to replicate these analyses please consult the Fragile Families and Child Wellbeing Survey [website](https://fragilefamilies.princeton.edu/documentation).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('../../../FFChallenge_v2/train.csv',low_memory=False, index_col='challengeID')\n",
    "predictions=pd.read_csv('../../../FFChallenge_v2/prediction.csv',low_memory=False, index_col='challengeID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a version of the data with missing values imputed (`full_imputed.csv`) the script `clean_files.py` must first be run. If necessary it can be executed by uncommenting (deleting the #) and running the line below. ***This script will take approximately 30 minutes to run. It only needs to be run once.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! PYTHONHASHSEED=0 python3 ../preprocess/clean_files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/full_imputed.csv') # load imputed data output after running the clean_files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4242, 4569)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index = data['challengeID']\n",
    "del data['challengeID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m1lenmin</th>\n",
       "      <th>m1citywt</th>\n",
       "      <th>m1e1d1</th>\n",
       "      <th>m1e1d2</th>\n",
       "      <th>m1e1d3</th>\n",
       "      <th>m1i2a</th>\n",
       "      <th>m1i2b</th>\n",
       "      <th>m1j2a</th>\n",
       "      <th>m1j2b</th>\n",
       "      <th>cm1hhinc</th>\n",
       "      <th>...</th>\n",
       "      <th>hv4mflag</th>\n",
       "      <th>hv4mompreg</th>\n",
       "      <th>hv4selfht</th>\n",
       "      <th>hv4selfwt</th>\n",
       "      <th>gpa</th>\n",
       "      <th>grit</th>\n",
       "      <th>materialHardship</th>\n",
       "      <th>eviction</th>\n",
       "      <th>layoff</th>\n",
       "      <th>jobTraining</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>challengeID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.0</td>\n",
       "      <td>202.485367</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.723174</td>\n",
       "      <td>13.260396</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1682.415602</td>\n",
       "      <td>0.038262</td>\n",
       "      <td>2.211822</td>\n",
       "      <td>29579.694329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.0</td>\n",
       "      <td>45.608219</td>\n",
       "      <td>43.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3050.504448</td>\n",
       "      <td>0.110909</td>\n",
       "      <td>1.985703</td>\n",
       "      <td>20829.093487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>39.060299</td>\n",
       "      <td>49.0</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.158179</td>\n",
       "      <td>1.386592</td>\n",
       "      <td>132483.450592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.0</td>\n",
       "      <td>22.304855</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.169628</td>\n",
       "      <td>5.699719</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.165048</td>\n",
       "      <td>1.157385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25.0</td>\n",
       "      <td>35.518272</td>\n",
       "      <td>90.0</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1974.812374</td>\n",
       "      <td>12.212538</td>\n",
       "      <td>2.965919</td>\n",
       "      <td>49026.982561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4568 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             m1lenmin    m1citywt  m1e1d1     m1e1d2     m1e1d3  m1i2a  \\\n",
       "challengeID                                                              \n",
       "1                40.0  202.485367    25.0   6.723174  13.260396   38.0   \n",
       "2                40.0   45.608219    43.0  16.000000   3.000000   25.0   \n",
       "3                35.0   39.060299    49.0  46.000000  23.000000   20.0   \n",
       "4                30.0   22.304855    23.0  23.169628   5.699719   20.0   \n",
       "5                25.0   35.518272    90.0  64.000000  58.000000   12.0   \n",
       "\n",
       "                   m1i2b      m1j2a     m1j2b       cm1hhinc     ...       \\\n",
       "challengeID                                                      ...        \n",
       "1            1682.415602   0.038262  2.211822   29579.694329     ...        \n",
       "2            3050.504448   0.110909  1.985703   20829.093487     ...        \n",
       "3               0.000000  12.158179  1.386592  132483.450592     ...        \n",
       "4               0.000000   4.165048  1.157385       0.000000     ...        \n",
       "5            1974.812374  12.212538  2.965919   49026.982561     ...        \n",
       "\n",
       "             hv4mflag  hv4mompreg  hv4selfht  hv4selfwt  gpa  grit  \\\n",
       "challengeID                                                          \n",
       "1                 0.0         0.0        0.0        0.0  2.5  3.50   \n",
       "2                 0.0         1.0        0.0        0.0  2.5  3.25   \n",
       "3                 0.0         0.0        0.0        0.0  2.5  3.00   \n",
       "4                 0.0         0.0        0.0        0.0  2.5  3.50   \n",
       "5                 0.0         0.0        0.0        0.0  2.5  3.50   \n",
       "\n",
       "             materialHardship  eviction  layoff  jobTraining  \n",
       "challengeID                                                   \n",
       "1                    0.090909       0.0     0.0          0.0  \n",
       "2                    0.181818       0.0     0.0          0.0  \n",
       "3                    0.000000       0.0     0.0          0.0  \n",
       "4                    0.090909       0.0     0.0          0.0  \n",
       "5                    0.090909       0.0     0.0          0.0  \n",
       "\n",
       "[5 rows x 4568 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the outcomes from the imputed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[['gpa','grit','materialHardship','eviction','layoff','jobTraining']]\n",
    "X = data\n",
    "for c in X.columns:\n",
    "    if c in list(y.columns):\n",
    "        del X[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "\n",
    "Before modelling the data there are two types of transformations that I use to optimize them for the neural network.\n",
    "\n",
    "Categorical variables are transformed using one-hot encoding. Continuous variables are also normalized to have a mean of zero.\n",
    "\n",
    "To identify which columns belong to which group I use same heuristic as in the imputation script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "cat_cols = []\n",
    "non_cat_cols = []\n",
    "for i, c in enumerate(X.columns):\n",
    "    is_categorical = False\n",
    "    vals = set(list(X[c]))\n",
    "    vals = {x for x in vals if x==x} # Removes nans, otherwise treated as unique\n",
    "    if X[c].dtype == 'float64': # if float and low num distinct then treat as cat\n",
    "        if len(vals) <= 20:\n",
    "            is_categorical = True\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        is_categorical = True\n",
    "    \n",
    "    # Now append to relevant list of columns\n",
    "    if is_categorical:\n",
    "        cat_cols.append(c)\n",
    "        \n",
    "    else:\n",
    "        non_cat_cols.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dummies = pd.get_dummies(X, columns=cat_cols)\n",
    "# Note that sklearn also has one-hot encoding but doesn't relabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m1lenmin</th>\n",
       "      <th>m1citywt</th>\n",
       "      <th>m1e1d1</th>\n",
       "      <th>m1e1d2</th>\n",
       "      <th>m1e1d3</th>\n",
       "      <th>m1i2a</th>\n",
       "      <th>m1i2b</th>\n",
       "      <th>m1j2a</th>\n",
       "      <th>m1j2b</th>\n",
       "      <th>cm1hhinc</th>\n",
       "      <th>...</th>\n",
       "      <th>hv4mflag_1.0</th>\n",
       "      <th>hv4mflag_2.0</th>\n",
       "      <th>hv4mflag_3.0</th>\n",
       "      <th>hv4mompreg_0.0</th>\n",
       "      <th>hv4mompreg_1.0</th>\n",
       "      <th>hv4selfht_0.0</th>\n",
       "      <th>hv4selfht_1.0</th>\n",
       "      <th>hv4selfht_2.0</th>\n",
       "      <th>hv4selfwt_0.0</th>\n",
       "      <th>hv4selfwt_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>challengeID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.0</td>\n",
       "      <td>202.485367</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.723174</td>\n",
       "      <td>13.260396</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1682.415602</td>\n",
       "      <td>0.038262</td>\n",
       "      <td>2.211822</td>\n",
       "      <td>29579.694329</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.0</td>\n",
       "      <td>45.608219</td>\n",
       "      <td>43.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3050.504448</td>\n",
       "      <td>0.110909</td>\n",
       "      <td>1.985703</td>\n",
       "      <td>20829.093487</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>39.060299</td>\n",
       "      <td>49.0</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.158179</td>\n",
       "      <td>1.386592</td>\n",
       "      <td>132483.450592</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.0</td>\n",
       "      <td>22.304855</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.169628</td>\n",
       "      <td>5.699719</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.165048</td>\n",
       "      <td>1.157385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25.0</td>\n",
       "      <td>35.518272</td>\n",
       "      <td>90.0</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1974.812374</td>\n",
       "      <td>12.212538</td>\n",
       "      <td>2.965919</td>\n",
       "      <td>49026.982561</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16391 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             m1lenmin    m1citywt  m1e1d1     m1e1d2     m1e1d3  m1i2a  \\\n",
       "challengeID                                                              \n",
       "1                40.0  202.485367    25.0   6.723174  13.260396   38.0   \n",
       "2                40.0   45.608219    43.0  16.000000   3.000000   25.0   \n",
       "3                35.0   39.060299    49.0  46.000000  23.000000   20.0   \n",
       "4                30.0   22.304855    23.0  23.169628   5.699719   20.0   \n",
       "5                25.0   35.518272    90.0  64.000000  58.000000   12.0   \n",
       "\n",
       "                   m1i2b      m1j2a     m1j2b       cm1hhinc      ...        \\\n",
       "challengeID                                                       ...         \n",
       "1            1682.415602   0.038262  2.211822   29579.694329      ...         \n",
       "2            3050.504448   0.110909  1.985703   20829.093487      ...         \n",
       "3               0.000000  12.158179  1.386592  132483.450592      ...         \n",
       "4               0.000000   4.165048  1.157385       0.000000      ...         \n",
       "5            1974.812374  12.212538  2.965919   49026.982561      ...         \n",
       "\n",
       "             hv4mflag_1.0  hv4mflag_2.0  hv4mflag_3.0  hv4mompreg_0.0  \\\n",
       "challengeID                                                             \n",
       "1                       0             0             0               1   \n",
       "2                       0             0             0               0   \n",
       "3                       0             0             0               1   \n",
       "4                       0             0             0               1   \n",
       "5                       0             0             0               1   \n",
       "\n",
       "             hv4mompreg_1.0  hv4selfht_0.0  hv4selfht_1.0  hv4selfht_2.0  \\\n",
       "challengeID                                                                \n",
       "1                         0              1              0              0   \n",
       "2                         1              1              0              0   \n",
       "3                         0              1              0              0   \n",
       "4                         0              1              0              0   \n",
       "5                         0              1              0              0   \n",
       "\n",
       "             hv4selfwt_0.0  hv4selfwt_1.0  \n",
       "challengeID                                \n",
       "1                        1              0  \n",
       "2                        1              0  \n",
       "3                        1              0  \n",
       "4                        1              0  \n",
       "5                        1              0  \n",
       "\n",
       "[5 rows x 16391 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = StandardScaler()\n",
    "for c in non_cat_cols:\n",
    "    normed = normalizer.fit_transform(X_dummies[c].values.reshape(-1,1))\n",
    "    X_dummies[c] = normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m1lenmin</th>\n",
       "      <th>m1citywt</th>\n",
       "      <th>m1e1d1</th>\n",
       "      <th>m1e1d2</th>\n",
       "      <th>m1e1d3</th>\n",
       "      <th>m1i2a</th>\n",
       "      <th>m1i2b</th>\n",
       "      <th>m1j2a</th>\n",
       "      <th>m1j2b</th>\n",
       "      <th>cm1hhinc</th>\n",
       "      <th>...</th>\n",
       "      <th>hv4mflag_1.0</th>\n",
       "      <th>hv4mflag_2.0</th>\n",
       "      <th>hv4mflag_3.0</th>\n",
       "      <th>hv4mompreg_0.0</th>\n",
       "      <th>hv4mompreg_1.0</th>\n",
       "      <th>hv4selfht_0.0</th>\n",
       "      <th>hv4selfht_1.0</th>\n",
       "      <th>hv4selfht_2.0</th>\n",
       "      <th>hv4selfwt_0.0</th>\n",
       "      <th>hv4selfwt_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>challengeID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.364267</td>\n",
       "      <td>0.675623</td>\n",
       "      <td>-0.545055</td>\n",
       "      <td>-0.788818</td>\n",
       "      <td>0.183309</td>\n",
       "      <td>0.255698</td>\n",
       "      <td>-0.245954</td>\n",
       "      <td>-1.512365</td>\n",
       "      <td>0.017524</td>\n",
       "      <td>-0.105330</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.364267</td>\n",
       "      <td>-0.197911</td>\n",
       "      <td>0.606432</td>\n",
       "      <td>-0.149595</td>\n",
       "      <td>-1.041256</td>\n",
       "      <td>-0.986483</td>\n",
       "      <td>-0.080941</td>\n",
       "      <td>-1.497946</td>\n",
       "      <td>-0.168142</td>\n",
       "      <td>-0.377045</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.071568</td>\n",
       "      <td>-0.234372</td>\n",
       "      <td>0.990261</td>\n",
       "      <td>1.917563</td>\n",
       "      <td>1.345718</td>\n",
       "      <td>-1.464245</td>\n",
       "      <td>-0.448880</td>\n",
       "      <td>0.893159</td>\n",
       "      <td>-0.660072</td>\n",
       "      <td>3.089933</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.507403</td>\n",
       "      <td>-0.327670</td>\n",
       "      <td>-0.672998</td>\n",
       "      <td>0.344430</td>\n",
       "      <td>-0.719048</td>\n",
       "      <td>-1.464245</td>\n",
       "      <td>-0.448880</td>\n",
       "      <td>-0.693293</td>\n",
       "      <td>-0.848273</td>\n",
       "      <td>-1.023809</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.943238</td>\n",
       "      <td>-0.254094</td>\n",
       "      <td>3.613094</td>\n",
       "      <td>3.157858</td>\n",
       "      <td>5.522921</td>\n",
       "      <td>-2.228664</td>\n",
       "      <td>-0.210687</td>\n",
       "      <td>0.903948</td>\n",
       "      <td>0.636712</td>\n",
       "      <td>0.498527</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16391 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             m1lenmin  m1citywt    m1e1d1    m1e1d2    m1e1d3     m1i2a  \\\n",
       "challengeID                                                               \n",
       "1            0.364267  0.675623 -0.545055 -0.788818  0.183309  0.255698   \n",
       "2            0.364267 -0.197911  0.606432 -0.149595 -1.041256 -0.986483   \n",
       "3           -0.071568 -0.234372  0.990261  1.917563  1.345718 -1.464245   \n",
       "4           -0.507403 -0.327670 -0.672998  0.344430 -0.719048 -1.464245   \n",
       "5           -0.943238 -0.254094  3.613094  3.157858  5.522921 -2.228664   \n",
       "\n",
       "                m1i2b     m1j2a     m1j2b  cm1hhinc      ...        \\\n",
       "challengeID                                              ...         \n",
       "1           -0.245954 -1.512365  0.017524 -0.105330      ...         \n",
       "2           -0.080941 -1.497946 -0.168142 -0.377045      ...         \n",
       "3           -0.448880  0.893159 -0.660072  3.089933      ...         \n",
       "4           -0.448880 -0.693293 -0.848273 -1.023809      ...         \n",
       "5           -0.210687  0.903948  0.636712  0.498527      ...         \n",
       "\n",
       "             hv4mflag_1.0  hv4mflag_2.0  hv4mflag_3.0  hv4mompreg_0.0  \\\n",
       "challengeID                                                             \n",
       "1                       0             0             0               1   \n",
       "2                       0             0             0               0   \n",
       "3                       0             0             0               1   \n",
       "4                       0             0             0               1   \n",
       "5                       0             0             0               1   \n",
       "\n",
       "             hv4mompreg_1.0  hv4selfht_0.0  hv4selfht_1.0  hv4selfht_2.0  \\\n",
       "challengeID                                                                \n",
       "1                         0              1              0              0   \n",
       "2                         1              1              0              0   \n",
       "3                         0              1              0              0   \n",
       "4                         0              1              0              0   \n",
       "5                         0              1              0              0   \n",
       "\n",
       "             hv4selfwt_0.0  hv4selfwt_1.0  \n",
       "challengeID                                \n",
       "1                        1              0  \n",
       "2                        1              0  \n",
       "3                        1              0  \n",
       "4                        1              0  \n",
       "5                        1              0  \n",
       "\n",
       "[5 rows x 16391 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_dummies # rename X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now splitting the X and y matrices to separate cases in the training set and the prediction set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training=X.loc[X.index.isin(train.index)]\n",
    "X_pred=X.loc[~X.index.isin(train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_training=y.loc[y.index.isin(train.index)]\n",
    "y_pred=y.loc[~y.index.isin(train.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly splitting the data into training and test sets, where 20% of data is held out for validation and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_training, y_training.gpa, test_size=0.20, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing these files for later\n",
    "X_test.to_csv('../../data/X_test.csv')\n",
    "pd.Series(cat_cols).to_csv('../../data/cat_cols.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a function that can be used to return Keras models with different parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(activation_function, num_hidden_layers, hidden_layer_size):\n",
    "    '''\n",
    "    A function to create a Keras sequential model based on input parameters.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "        activation_function: str\n",
    "            Activation function to be used in model.\n",
    "        \n",
    "        num_hidden_layers: int\n",
    "            Number of hidden layers in model\n",
    "        \n",
    "        hidden_layer_size: int\n",
    "            Number of units/neurons in each hidden layer\n",
    "            \n",
    "    Returns\n",
    "    ---------\n",
    "    \n",
    "    Keras Sequential model object\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    \n",
    "    # Single layer model\n",
    "    if num_hidden_layers == 0: # then just specify a single layer, 1 is size of output\n",
    "        model.add(Dense(1, \n",
    "                        input_dim=X_train.shape[1], \n",
    "                        activation=activation_function,\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer=glorot_uniform(seed=seed)\n",
    "                      ))\n",
    "        \n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "    # Specify initial layer with a hidden layer\n",
    "    if num_hidden_layers >= 1: \n",
    "        model.add(Dense(hidden_layer_size, \n",
    "                        input_dim=X_train.shape[1], \n",
    "                        activation=activation_function,\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer=glorot_uniform(seed=seed)\n",
    "                       ))\n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "    # Now add additional hidden layers\n",
    "    for i in range(0,num_hidden_layers-1):\n",
    "        model.add(Dense(hidden_layer_size, \n",
    "                        activation=activation_function, \n",
    "                        use_bias=True,\n",
    "                        kernel_initializer=glorot_uniform(seed=seed)\n",
    "                       ))\n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "    if num_hidden_layers > 0:       \n",
    "        model.add(Dense(1)) # Final output layer, don't add if no hidden layers\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I take the model object and use it to initialize a classifier using the scikit-learn Keras wrapped object `KerasRegressor`.\n",
    "\n",
    "I then define the parameter space to search over and pass both to a `GridSearchCV` object. \n",
    "\n",
    "Once the `fit` method is called the grid search will begin and a model will fit for every parameter combination and fold (40 x 5). \n",
    "\n",
    "***Note: This will take 12 hours or more to complete***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1356 samples, validate on 425 samples\n",
      "Epoch 1/200\n",
      "1356/1356 [==============================] - 1s 908us/step - loss: 2.1338 - val_loss: 0.4148\n",
      "Epoch 2/200\n",
      "1356/1356 [==============================] - 1s 613us/step - loss: 1.5126 - val_loss: 0.6434\n",
      "Epoch 3/200\n",
      "1356/1356 [==============================] - 1s 633us/step - loss: 1.2666 - val_loss: 0.7307\n",
      "Epoch 4/200\n",
      "1356/1356 [==============================] - 1s 583us/step - loss: 1.1299 - val_loss: 0.7382\n",
      "Epoch 5/200\n",
      "1356/1356 [==============================] - 1s 582us/step - loss: 1.0447 - val_loss: 0.6972\n",
      "Epoch 6/200\n",
      "1356/1356 [==============================] - 1s 620us/step - loss: 0.9432 - val_loss: 1.1264\n",
      "Epoch 7/200\n",
      "1356/1356 [==============================] - 1s 613us/step - loss: 0.8394 - val_loss: 0.9135\n",
      "Epoch 8/200\n",
      "1356/1356 [==============================] - 1s 613us/step - loss: 0.7951 - val_loss: 0.8134\n",
      "Epoch 9/200\n",
      "1356/1356 [==============================] - 1s 609us/step - loss: 0.7321 - val_loss: 0.8836\n",
      "Epoch 10/200\n",
      "1356/1356 [==============================] - 1s 594us/step - loss: 0.6871 - val_loss: 0.6532\n",
      "Epoch 11/200\n",
      "1356/1356 [==============================] - 1s 620us/step - loss: 0.6313 - val_loss: 0.6260\n",
      "Epoch 12/200\n",
      "1356/1356 [==============================] - 1s 615us/step - loss: 0.5542 - val_loss: 0.7131\n",
      "Epoch 13/200\n",
      "1356/1356 [==============================] - 1s 631us/step - loss: 0.5553 - val_loss: 0.6600\n",
      "Epoch 14/200\n",
      "1356/1356 [==============================] - 1s 679us/step - loss: 0.5401 - val_loss: 0.5634\n",
      "Epoch 15/200\n",
      "1356/1356 [==============================] - 1s 636us/step - loss: 0.5264 - val_loss: 0.4776\n",
      "Epoch 16/200\n",
      "1356/1356 [==============================] - 1s 606us/step - loss: 0.5410 - val_loss: 0.4113\n",
      "Epoch 17/200\n",
      "1356/1356 [==============================] - 1s 588us/step - loss: 0.4973 - val_loss: 0.4499\n",
      "Epoch 18/200\n",
      "1356/1356 [==============================] - 1s 581us/step - loss: 0.5025 - val_loss: 0.4081\n",
      "Epoch 19/200\n",
      "1356/1356 [==============================] - 1s 602us/step - loss: 0.4788 - val_loss: 0.3901\n",
      "Epoch 20/200\n",
      "1356/1356 [==============================] - 1s 609us/step - loss: 0.4566 - val_loss: 0.3728\n",
      "Epoch 21/200\n",
      "1356/1356 [==============================] - 1s 595us/step - loss: 0.4747 - val_loss: 0.3554\n",
      "Epoch 22/200\n",
      "1356/1356 [==============================] - 1s 608us/step - loss: 0.4424 - val_loss: 0.3348\n",
      "Epoch 23/200\n",
      "1356/1356 [==============================] - 1s 596us/step - loss: 0.4585 - val_loss: 0.3487\n",
      "Epoch 24/200\n",
      "1356/1356 [==============================] - 1s 610us/step - loss: 0.4583 - val_loss: 0.3304\n",
      "Epoch 25/200\n",
      "1356/1356 [==============================] - 1s 593us/step - loss: 0.4484 - val_loss: 0.3412\n",
      "Epoch 26/200\n",
      "1356/1356 [==============================] - 1s 578us/step - loss: 0.4494 - val_loss: 0.3370\n",
      "Epoch 27/200\n",
      "1356/1356 [==============================] - 1s 583us/step - loss: 0.4535 - val_loss: 0.3091\n",
      "Epoch 28/200\n",
      "1356/1356 [==============================] - 1s 623us/step - loss: 0.4543 - val_loss: 0.3261\n",
      "Epoch 29/200\n",
      "1356/1356 [==============================] - 1s 582us/step - loss: 0.4251 - val_loss: 0.3189\n",
      "Epoch 30/200\n",
      "1356/1356 [==============================] - 1s 596us/step - loss: 0.4423 - val_loss: 0.3165\n",
      "Epoch 31/200\n",
      "1356/1356 [==============================] - 1s 628us/step - loss: 0.4612 - val_loss: 0.3139\n",
      "Epoch 32/200\n",
      "1356/1356 [==============================] - 1s 618us/step - loss: 0.4271 - val_loss: 0.3085\n",
      "Epoch 33/200\n",
      "1356/1356 [==============================] - 1s 580us/step - loss: 0.4239 - val_loss: 0.3161\n",
      "Epoch 34/200\n",
      "1356/1356 [==============================] - 1s 557us/step - loss: 0.4368 - val_loss: 0.3107\n",
      "Epoch 35/200\n",
      "1356/1356 [==============================] - 1s 560us/step - loss: 0.4432 - val_loss: 0.3094\n",
      "Epoch 36/200\n",
      "1356/1356 [==============================] - 1s 565us/step - loss: 0.4091 - val_loss: 0.3077\n",
      "Epoch 37/200\n",
      "1356/1356 [==============================] - 1s 583us/step - loss: 0.4333 - val_loss: 0.3077\n",
      "Epoch 38/200\n",
      "1356/1356 [==============================] - 1s 584us/step - loss: 0.4318 - val_loss: 0.3078\n",
      "Epoch 39/200\n",
      "1356/1356 [==============================] - 1s 620us/step - loss: 0.4109 - val_loss: 0.3141\n",
      "Epoch 40/200\n",
      "1356/1356 [==============================] - 1s 607us/step - loss: 0.4185 - val_loss: 0.3076\n",
      "Epoch 41/200\n",
      "1356/1356 [==============================] - 1s 596us/step - loss: 0.4163 - val_loss: 0.3078\n",
      "Epoch 42/200\n",
      "1356/1356 [==============================] - 1s 579us/step - loss: 0.4134 - val_loss: 0.3082\n",
      "Epoch 43/200\n",
      "1356/1356 [==============================] - 1s 599us/step - loss: 0.4093 - val_loss: 0.3083\n",
      "Epoch 44/200\n",
      "1356/1356 [==============================] - 1s 594us/step - loss: 0.4039 - val_loss: 0.3076\n",
      "Epoch 45/200\n",
      "1356/1356 [==============================] - 1s 643us/step - loss: 0.4087 - val_loss: 0.3095\n",
      "Epoch 46/200\n",
      "1356/1356 [==============================] - 1s 1ms/step - loss: 0.4163 - val_loss: 0.3150\n",
      "Epoch 47/200\n",
      "1356/1356 [==============================] - 1s 831us/step - loss: 0.3936 - val_loss: 0.3116\n",
      "Epoch 48/200\n",
      "1356/1356 [==============================] - 1s 952us/step - loss: 0.4084 - val_loss: 0.3077\n",
      "Epoch 49/200\n",
      "1356/1356 [==============================] - 1s 762us/step - loss: 0.4155 - val_loss: 0.3128\n",
      "Epoch 50/200\n",
      "1356/1356 [==============================] - 1s 559us/step - loss: 0.3974 - val_loss: 0.3122\n",
      "Epoch 51/200\n",
      "1356/1356 [==============================] - 1s 555us/step - loss: 0.4001 - val_loss: 0.3086\n",
      "Epoch 52/200\n",
      "1356/1356 [==============================] - 1s 554us/step - loss: 0.3954 - val_loss: 0.3084\n",
      "Epoch 53/200\n",
      "1356/1356 [==============================] - 1s 555us/step - loss: 0.3924 - val_loss: 0.3099\n",
      "Epoch 54/200\n",
      "1356/1356 [==============================] - 1s 564us/step - loss: 0.4042 - val_loss: 0.3106\n",
      "Epoch 55/200\n",
      "1356/1356 [==============================] - 1s 609us/step - loss: 0.3967 - val_loss: 0.3076\n",
      "Epoch 56/200\n",
      "1356/1356 [==============================] - 1s 547us/step - loss: 0.3917 - val_loss: 0.3076\n",
      "Epoch 57/200\n",
      "1356/1356 [==============================] - 1s 555us/step - loss: 0.3891 - val_loss: 0.3076\n",
      "Epoch 58/200\n",
      "1356/1356 [==============================] - 1s 611us/step - loss: 0.4086 - val_loss: 0.3125\n",
      "Epoch 59/200\n",
      "1356/1356 [==============================] - 1s 586us/step - loss: 0.3973 - val_loss: 0.3099\n",
      "Epoch 60/200\n",
      "1356/1356 [==============================] - 1s 580us/step - loss: 0.3926 - val_loss: 0.3115\n",
      "Epoch 61/200\n",
      "1356/1356 [==============================] - 1s 603us/step - loss: 0.3737 - val_loss: 0.3078\n",
      "Epoch 00061: early stopping\n",
      "[CV]  activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2, total=  53.2s\n",
      "[CV] activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   53.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1357 samples, validate on 425 samples\n",
      "Epoch 1/200\n",
      "1357/1357 [==============================] - 1s 856us/step - loss: 1.7802 - val_loss: 0.4941\n",
      "Epoch 2/200\n",
      "1357/1357 [==============================] - 1s 573us/step - loss: 1.3153 - val_loss: 0.9173\n",
      "Epoch 3/200\n",
      "1357/1357 [==============================] - 1s 577us/step - loss: 1.1157 - val_loss: 0.7246\n",
      "Epoch 4/200\n",
      "1357/1357 [==============================] - 1s 573us/step - loss: 0.9828 - val_loss: 0.7033\n",
      "Epoch 5/200\n",
      "1357/1357 [==============================] - 1s 643us/step - loss: 0.9046 - val_loss: 0.9277\n",
      "Epoch 6/200\n",
      "1357/1357 [==============================] - 1s 624us/step - loss: 0.8063 - val_loss: 0.9745\n",
      "Epoch 7/200\n",
      "1357/1357 [==============================] - 1s 607us/step - loss: 0.7170 - val_loss: 0.7623\n",
      "Epoch 8/200\n",
      "1357/1357 [==============================] - 1s 598us/step - loss: 0.6844 - val_loss: 0.7032\n",
      "Epoch 9/200\n",
      "1357/1357 [==============================] - 1s 560us/step - loss: 0.6440 - val_loss: 0.6174\n",
      "Epoch 10/200\n",
      "1357/1357 [==============================] - 1s 558us/step - loss: 0.6300 - val_loss: 0.5942\n",
      "Epoch 11/200\n",
      "1357/1357 [==============================] - 1s 569us/step - loss: 0.5958 - val_loss: 0.5355\n",
      "Epoch 12/200\n",
      "1357/1357 [==============================] - 1s 566us/step - loss: 0.5848 - val_loss: 0.4913\n",
      "Epoch 13/200\n",
      "1357/1357 [==============================] - 1s 605us/step - loss: 0.5179 - val_loss: 0.4238\n",
      "Epoch 14/200\n",
      "1357/1357 [==============================] - 1s 641us/step - loss: 0.5484 - val_loss: 0.4052\n",
      "Epoch 15/200\n",
      "1357/1357 [==============================] - 1s 561us/step - loss: 0.5026 - val_loss: 0.3653\n",
      "Epoch 16/200\n",
      "1357/1357 [==============================] - 1s 641us/step - loss: 0.5050 - val_loss: 0.3895\n",
      "Epoch 17/200\n",
      "1357/1357 [==============================] - 1s 643us/step - loss: 0.5000 - val_loss: 0.3909\n",
      "Epoch 18/200\n",
      "1357/1357 [==============================] - 1s 581us/step - loss: 0.4672 - val_loss: 0.3467\n",
      "Epoch 19/200\n",
      "1357/1357 [==============================] - 1s 576us/step - loss: 0.4656 - val_loss: 0.3396\n",
      "Epoch 20/200\n",
      "1357/1357 [==============================] - 1s 555us/step - loss: 0.4818 - val_loss: 0.3120\n",
      "Epoch 21/200\n",
      "1357/1357 [==============================] - 1s 568us/step - loss: 0.4506 - val_loss: 0.3181\n",
      "Epoch 22/200\n",
      "1357/1357 [==============================] - 1s 564us/step - loss: 0.4387 - val_loss: 0.3292\n",
      "Epoch 23/200\n",
      "1357/1357 [==============================] - 1s 598us/step - loss: 0.4729 - val_loss: 0.3160\n",
      "Epoch 24/200\n",
      "1357/1357 [==============================] - 1s 655us/step - loss: 0.4453 - val_loss: 0.3109\n",
      "Epoch 25/200\n",
      "1357/1357 [==============================] - 1s 876us/step - loss: 0.4376 - val_loss: 0.3102\n",
      "Epoch 26/200\n",
      "1357/1357 [==============================] - 1s 572us/step - loss: 0.4492 - val_loss: 0.3076\n",
      "Epoch 27/200\n",
      "1357/1357 [==============================] - 1s 665us/step - loss: 0.4330 - val_loss: 0.3100\n",
      "Epoch 28/200\n",
      "1357/1357 [==============================] - 1s 638us/step - loss: 0.4462 - val_loss: 0.3145\n",
      "Epoch 29/200\n",
      "1357/1357 [==============================] - 1s 578us/step - loss: 0.4405 - val_loss: 0.3086\n",
      "Epoch 30/200\n",
      "1357/1357 [==============================] - 1s 653us/step - loss: 0.4311 - val_loss: 0.3089\n",
      "Epoch 31/200\n",
      "1357/1357 [==============================] - 1s 619us/step - loss: 0.4501 - val_loss: 0.3104\n",
      "Epoch 32/200\n",
      "1357/1357 [==============================] - 1s 615us/step - loss: 0.4446 - val_loss: 0.3148\n",
      "Epoch 33/200\n",
      "1357/1357 [==============================] - 1s 642us/step - loss: 0.4124 - val_loss: 0.3141\n",
      "Epoch 34/200\n",
      "1357/1357 [==============================] - 1s 701us/step - loss: 0.4253 - val_loss: 0.3097\n",
      "Epoch 35/200\n",
      "1357/1357 [==============================] - 1s 982us/step - loss: 0.4245 - val_loss: 0.3150\n",
      "Epoch 36/200\n",
      "1357/1357 [==============================] - 1s 800us/step - loss: 0.4288 - val_loss: 0.3076\n",
      "Epoch 37/200\n",
      "1357/1357 [==============================] - 1s 623us/step - loss: 0.4138 - val_loss: 0.3121\n",
      "Epoch 38/200\n",
      "1357/1357 [==============================] - 1s 700us/step - loss: 0.4135 - val_loss: 0.3154\n",
      "Epoch 39/200\n",
      "1357/1357 [==============================] - 1s 905us/step - loss: 0.4095 - val_loss: 0.3079\n",
      "Epoch 40/200\n",
      "1357/1357 [==============================] - 1s 1ms/step - loss: 0.4364 - val_loss: 0.3087\n",
      "Epoch 41/200\n",
      "1357/1357 [==============================] - 1s 839us/step - loss: 0.4296 - val_loss: 0.3076\n",
      "Epoch 42/200\n",
      "1357/1357 [==============================] - 1s 827us/step - loss: 0.4094 - val_loss: 0.3087\n",
      "Epoch 43/200\n",
      "1357/1357 [==============================] - 1s 1ms/step - loss: 0.4016 - val_loss: 0.3076\n",
      "Epoch 44/200\n",
      "1357/1357 [==============================] - 2s 1ms/step - loss: 0.3959 - val_loss: 0.3219\n",
      "Epoch 45/200\n",
      "1357/1357 [==============================] - 1s 854us/step - loss: 0.4037 - val_loss: 0.3077\n",
      "Epoch 46/200\n",
      "1357/1357 [==============================] - 1s 709us/step - loss: 0.3986 - val_loss: 0.3081\n",
      "Epoch 47/200\n",
      "1357/1357 [==============================] - 1s 770us/step - loss: 0.4017 - val_loss: 0.3089\n",
      "Epoch 48/200\n",
      "1357/1357 [==============================] - 1s 729us/step - loss: 0.4032 - val_loss: 0.3199\n",
      "Epoch 49/200\n",
      "1357/1357 [==============================] - 1s 685us/step - loss: 0.4105 - val_loss: 0.3199\n",
      "Epoch 50/200\n",
      "1357/1357 [==============================] - 1s 702us/step - loss: 0.4071 - val_loss: 0.3216\n",
      "Epoch 51/200\n",
      "1357/1357 [==============================] - 1s 692us/step - loss: 0.4073 - val_loss: 0.3077\n",
      "Epoch 00051: early stopping\n",
      "[CV]  activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2, total=  49.5s\n",
      "[CV] activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2 \n",
      "Train on 1357 samples, validate on 425 samples\n",
      "Epoch 1/200\n",
      "1357/1357 [==============================] - 1s 1ms/step - loss: 1.8068 - val_loss: 0.4156\n",
      "Epoch 2/200\n",
      "1357/1357 [==============================] - 1s 931us/step - loss: 1.4393 - val_loss: 0.5862\n",
      "Epoch 3/200\n",
      "1357/1357 [==============================] - 1s 675us/step - loss: 1.2156 - val_loss: 0.5642\n",
      "Epoch 4/200\n",
      "1357/1357 [==============================] - 1s 711us/step - loss: 1.0423 - val_loss: 0.4947\n",
      "Epoch 5/200\n",
      "1357/1357 [==============================] - 1s 669us/step - loss: 0.9473 - val_loss: 0.7530\n",
      "Epoch 6/200\n",
      "1357/1357 [==============================] - 1s 699us/step - loss: 0.8522 - val_loss: 0.6417\n",
      "Epoch 7/200\n",
      "1357/1357 [==============================] - 1s 688us/step - loss: 0.7717 - val_loss: 0.5913\n",
      "Epoch 8/200\n",
      "1357/1357 [==============================] - 1s 695us/step - loss: 0.7270 - val_loss: 0.6048\n",
      "Epoch 9/200\n",
      "1357/1357 [==============================] - 1s 687us/step - loss: 0.6675 - val_loss: 0.6508\n",
      "Epoch 10/200\n",
      "1357/1357 [==============================] - 1s 661us/step - loss: 0.6076 - val_loss: 0.6210\n",
      "Epoch 11/200\n",
      "1357/1357 [==============================] - 1s 784us/step - loss: 0.5756 - val_loss: 0.6397\n",
      "Epoch 12/200\n",
      "1357/1357 [==============================] - 1s 830us/step - loss: 0.5686 - val_loss: 0.5489\n",
      "Epoch 13/200\n",
      "1357/1357 [==============================] - 1s 920us/step - loss: 0.5386 - val_loss: 0.4281\n",
      "Epoch 14/200\n",
      "1357/1357 [==============================] - 1s 682us/step - loss: 0.5108 - val_loss: 0.5188\n",
      "Epoch 15/200\n",
      "1357/1357 [==============================] - 1s 684us/step - loss: 0.5068 - val_loss: 0.4723\n",
      "Epoch 16/200\n",
      "1357/1357 [==============================] - 1s 713us/step - loss: 0.5051 - val_loss: 0.3699\n",
      "Epoch 17/200\n",
      "1357/1357 [==============================] - 1s 603us/step - loss: 0.5006 - val_loss: 0.3291\n",
      "Epoch 18/200\n",
      "1357/1357 [==============================] - 1s 604us/step - loss: 0.5003 - val_loss: 0.3587\n",
      "Epoch 19/200\n",
      "1357/1357 [==============================] - 1s 624us/step - loss: 0.4417 - val_loss: 0.3680\n",
      "Epoch 20/200\n",
      "1357/1357 [==============================] - 1s 700us/step - loss: 0.4679 - val_loss: 0.3419\n",
      "Epoch 21/200\n",
      "1357/1357 [==============================] - 1s 971us/step - loss: 0.4387 - val_loss: 0.3349\n",
      "Epoch 22/200\n",
      "1357/1357 [==============================] - 2s 1ms/step - loss: 0.4464 - val_loss: 0.3265\n",
      "Epoch 23/200\n",
      "1357/1357 [==============================] - 1s 743us/step - loss: 0.4551 - val_loss: 0.3033\n",
      "Epoch 24/200\n",
      "1357/1357 [==============================] - 1s 730us/step - loss: 0.4786 - val_loss: 0.3092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200\n",
      "1357/1357 [==============================] - 1s 640us/step - loss: 0.4267 - val_loss: 0.3083\n",
      "Epoch 26/200\n",
      "1357/1357 [==============================] - 1s 633us/step - loss: 0.4466 - val_loss: 0.3154\n",
      "Epoch 27/200\n",
      "1357/1357 [==============================] - 1s 606us/step - loss: 0.4130 - val_loss: 0.3028\n",
      "Epoch 28/200\n",
      "1357/1357 [==============================] - 1s 626us/step - loss: 0.4119 - val_loss: 0.3106\n",
      "Epoch 29/200\n",
      "1357/1357 [==============================] - 1s 637us/step - loss: 0.4197 - val_loss: 0.3103\n",
      "Epoch 30/200\n",
      "1357/1357 [==============================] - 1s 614us/step - loss: 0.4210 - val_loss: 0.3092\n",
      "Epoch 31/200\n",
      "1357/1357 [==============================] - 1s 617us/step - loss: 0.4024 - val_loss: 0.3031\n",
      "Epoch 32/200\n",
      "1357/1357 [==============================] - 1s 626us/step - loss: 0.4119 - val_loss: 0.3009\n",
      "Epoch 33/200\n",
      "1357/1357 [==============================] - 1s 653us/step - loss: 0.4060 - val_loss: 0.3056\n",
      "Epoch 34/200\n",
      "1357/1357 [==============================] - 1s 646us/step - loss: 0.4156 - val_loss: 0.3049\n",
      "Epoch 35/200\n",
      "1357/1357 [==============================] - 1s 609us/step - loss: 0.4095 - val_loss: 0.3009\n",
      "Epoch 36/200\n",
      "1357/1357 [==============================] - 1s 618us/step - loss: 0.4032 - val_loss: 0.2963\n",
      "Epoch 37/200\n",
      "1357/1357 [==============================] - 1s 652us/step - loss: 0.4284 - val_loss: 0.2988\n",
      "Epoch 38/200\n",
      "1357/1357 [==============================] - 1s 621us/step - loss: 0.4142 - val_loss: 0.3007\n",
      "Epoch 39/200\n",
      "1357/1357 [==============================] - 1s 600us/step - loss: 0.4183 - val_loss: 0.2993\n",
      "Epoch 40/200\n",
      "1357/1357 [==============================] - 1s 646us/step - loss: 0.4169 - val_loss: 0.2985\n",
      "Epoch 41/200\n",
      "1357/1357 [==============================] - 1s 631us/step - loss: 0.4038 - val_loss: 0.3124\n",
      "Epoch 42/200\n",
      "1357/1357 [==============================] - 1s 654us/step - loss: 0.3956 - val_loss: 0.2964\n",
      "Epoch 43/200\n",
      "1357/1357 [==============================] - 1s 633us/step - loss: 0.3811 - val_loss: 0.3006\n",
      "Epoch 44/200\n",
      "1357/1357 [==============================] - 1s 626us/step - loss: 0.3888 - val_loss: 0.2969\n",
      "Epoch 45/200\n",
      "1357/1357 [==============================] - 1s 612us/step - loss: 0.3735 - val_loss: 0.2966\n",
      "Epoch 46/200\n",
      "1357/1357 [==============================] - 1s 584us/step - loss: 0.3733 - val_loss: 0.3051\n",
      "Epoch 47/200\n",
      "1357/1357 [==============================] - 1s 589us/step - loss: 0.4125 - val_loss: 0.2938\n",
      "Epoch 48/200\n",
      "1357/1357 [==============================] - 1s 622us/step - loss: 0.3932 - val_loss: 0.2920\n",
      "Epoch 49/200\n",
      "1357/1357 [==============================] - 1s 672us/step - loss: 0.3996 - val_loss: 0.2923\n",
      "Epoch 50/200\n",
      "1357/1357 [==============================] - 1s 663us/step - loss: 0.3881 - val_loss: 0.3024\n",
      "Epoch 51/200\n",
      "1357/1357 [==============================] - 1s 686us/step - loss: 0.3671 - val_loss: 0.2923\n",
      "Epoch 52/200\n",
      "1357/1357 [==============================] - 1s 736us/step - loss: 0.3906 - val_loss: 0.2943\n",
      "Epoch 53/200\n",
      "1357/1357 [==============================] - 1s 757us/step - loss: 0.3678 - val_loss: 0.2927\n",
      "Epoch 54/200\n",
      "1357/1357 [==============================] - 1s 712us/step - loss: 0.3634 - val_loss: 0.3006\n",
      "Epoch 55/200\n",
      "1357/1357 [==============================] - 1s 756us/step - loss: 0.3731 - val_loss: 0.2891\n",
      "Epoch 56/200\n",
      "1357/1357 [==============================] - 1s 750us/step - loss: 0.3919 - val_loss: 0.2856\n",
      "Epoch 57/200\n",
      "1357/1357 [==============================] - 1s 687us/step - loss: 0.3726 - val_loss: 0.2910\n",
      "Epoch 58/200\n",
      "1357/1357 [==============================] - 1s 693us/step - loss: 0.3771 - val_loss: 0.2900\n",
      "Epoch 59/200\n",
      "1357/1357 [==============================] - 1s 713us/step - loss: 0.3700 - val_loss: 0.2931\n",
      "Epoch 60/200\n",
      "1357/1357 [==============================] - 1s 658us/step - loss: 0.3481 - val_loss: 0.3179\n",
      "Epoch 61/200\n",
      "1357/1357 [==============================] - 1s 869us/step - loss: 0.3668 - val_loss: 0.2830\n",
      "Epoch 62/200\n",
      "1357/1357 [==============================] - 1s 1ms/step - loss: 0.3621 - val_loss: 0.2873\n",
      "Epoch 63/200\n",
      "1357/1357 [==============================] - 1s 950us/step - loss: 0.3604 - val_loss: 0.2845\n",
      "Epoch 64/200\n",
      "1357/1357 [==============================] - 1s 768us/step - loss: 0.3425 - val_loss: 0.2898\n",
      "Epoch 65/200\n",
      "1357/1357 [==============================] - 1s 757us/step - loss: 0.3569 - val_loss: 0.2871\n",
      "Epoch 66/200\n",
      "1357/1357 [==============================] - 1s 897us/step - loss: 0.3370 - val_loss: 0.2865\n",
      "Epoch 67/200\n",
      "1357/1357 [==============================] - 1s 803us/step - loss: 0.3498 - val_loss: 0.2820\n",
      "Epoch 68/200\n",
      "1357/1357 [==============================] - 1s 761us/step - loss: 0.3383 - val_loss: 0.2985\n",
      "Epoch 69/200\n",
      "1357/1357 [==============================] - 1s 753us/step - loss: 0.3553 - val_loss: 0.2869\n",
      "Epoch 70/200\n",
      "1357/1357 [==============================] - 1s 742us/step - loss: 0.3357 - val_loss: 0.2873\n",
      "Epoch 71/200\n",
      "1357/1357 [==============================] - 1s 742us/step - loss: 0.3339 - val_loss: 0.2908\n",
      "Epoch 72/200\n",
      "1357/1357 [==============================] - 1s 744us/step - loss: 0.3280 - val_loss: 0.2921\n",
      "Epoch 73/200\n",
      "1357/1357 [==============================] - 1s 701us/step - loss: 0.3392 - val_loss: 0.3042\n",
      "Epoch 74/200\n",
      "1357/1357 [==============================] - 1s 671us/step - loss: 0.3452 - val_loss: 0.3112\n",
      "Epoch 75/200\n",
      "1357/1357 [==============================] - 1s 677us/step - loss: 0.3414 - val_loss: 0.3084\n",
      "Epoch 76/200\n",
      "1357/1357 [==============================] - 1s 700us/step - loss: 0.3392 - val_loss: 0.2849\n",
      "Epoch 77/200\n",
      "1357/1357 [==============================] - 1s 682us/step - loss: 0.3478 - val_loss: 0.3005\n",
      "Epoch 78/200\n",
      "1357/1357 [==============================] - 1s 702us/step - loss: 0.3375 - val_loss: 0.2938\n",
      "Epoch 79/200\n",
      "1357/1357 [==============================] - 1s 727us/step - loss: 0.3647 - val_loss: 0.2886\n",
      "Epoch 80/200\n",
      "1357/1357 [==============================] - 1s 711us/step - loss: 0.3312 - val_loss: 0.2911\n",
      "Epoch 81/200\n",
      "1357/1357 [==============================] - 1s 834us/step - loss: 0.3313 - val_loss: 0.2812\n",
      "Epoch 82/200\n",
      "1357/1357 [==============================] - 1s 780us/step - loss: 0.3133 - val_loss: 0.2782\n",
      "Epoch 83/200\n",
      "1357/1357 [==============================] - 1s 644us/step - loss: 0.3306 - val_loss: 0.2841\n",
      "Epoch 84/200\n",
      "1357/1357 [==============================] - 1s 731us/step - loss: 0.3412 - val_loss: 0.2948\n",
      "Epoch 85/200\n",
      "1357/1357 [==============================] - 1s 657us/step - loss: 0.3253 - val_loss: 0.2909\n",
      "Epoch 86/200\n",
      "1357/1357 [==============================] - 1s 740us/step - loss: 0.3384 - val_loss: 0.2879\n",
      "Epoch 87/200\n",
      "1357/1357 [==============================] - 1s 674us/step - loss: 0.3297 - val_loss: 0.3020\n",
      "Epoch 88/200\n",
      "1357/1357 [==============================] - 1s 667us/step - loss: 0.3102 - val_loss: 0.2853\n",
      "Epoch 89/200\n",
      "1357/1357 [==============================] - 1s 743us/step - loss: 0.3238 - val_loss: 0.2777\n",
      "Epoch 90/200\n",
      "1357/1357 [==============================] - 1s 765us/step - loss: 0.3392 - val_loss: 0.2885\n",
      "Epoch 91/200\n",
      "1357/1357 [==============================] - 1s 799us/step - loss: 0.3211 - val_loss: 0.2861\n",
      "Epoch 92/200\n",
      "1357/1357 [==============================] - 1s 688us/step - loss: 0.3258 - val_loss: 0.2920\n",
      "Epoch 93/200\n",
      "1357/1357 [==============================] - 1s 649us/step - loss: 0.3055 - val_loss: 0.2931\n",
      "Epoch 94/200\n",
      "1357/1357 [==============================] - 1s 650us/step - loss: 0.3111 - val_loss: 0.3042\n",
      "Epoch 95/200\n",
      "1357/1357 [==============================] - 1s 650us/step - loss: 0.3073 - val_loss: 0.2918\n",
      "Epoch 96/200\n",
      "1357/1357 [==============================] - 1s 658us/step - loss: 0.3107 - val_loss: 0.2917\n",
      "Epoch 97/200\n",
      "1357/1357 [==============================] - 1s 655us/step - loss: 0.3133 - val_loss: 0.2886\n",
      "Epoch 98/200\n",
      "1357/1357 [==============================] - 1s 660us/step - loss: 0.3020 - val_loss: 0.2933\n",
      "Epoch 99/200\n",
      "1357/1357 [==============================] - 1s 655us/step - loss: 0.2972 - val_loss: 0.2957\n",
      "Epoch 100/200\n",
      "1357/1357 [==============================] - 1s 605us/step - loss: 0.3122 - val_loss: 0.2971\n",
      "Epoch 101/200\n",
      "1357/1357 [==============================] - 1s 588us/step - loss: 0.3114 - val_loss: 0.2932\n",
      "Epoch 102/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1357/1357 [==============================] - 1s 561us/step - loss: 0.2906 - val_loss: 0.2955\n",
      "Epoch 103/200\n",
      "1357/1357 [==============================] - 1s 614us/step - loss: 0.3137 - val_loss: 0.2897\n",
      "Epoch 104/200\n",
      "1357/1357 [==============================] - 1s 639us/step - loss: 0.3085 - val_loss: 0.2803\n",
      "Epoch 105/200\n",
      "1357/1357 [==============================] - 1s 639us/step - loss: 0.3121 - val_loss: 0.2909\n",
      "Epoch 106/200\n",
      "1357/1357 [==============================] - 1s 634us/step - loss: 0.3105 - val_loss: 0.3052\n",
      "Epoch 107/200\n",
      "1357/1357 [==============================] - 1s 641us/step - loss: 0.3077 - val_loss: 0.2761\n",
      "Epoch 108/200\n",
      "1357/1357 [==============================] - 1s 638us/step - loss: 0.3095 - val_loss: 0.3310\n",
      "Epoch 109/200\n",
      "1357/1357 [==============================] - 1s 648us/step - loss: 0.3088 - val_loss: 0.2942\n",
      "Epoch 110/200\n",
      "1357/1357 [==============================] - 1s 637us/step - loss: 0.3118 - val_loss: 0.2854\n",
      "Epoch 111/200\n",
      "1357/1357 [==============================] - 1s 642us/step - loss: 0.2969 - val_loss: 0.2941\n",
      "Epoch 112/200\n",
      "1357/1357 [==============================] - 1s 642us/step - loss: 0.2940 - val_loss: 0.2932\n",
      "Epoch 113/200\n",
      "1357/1357 [==============================] - 1s 642us/step - loss: 0.2888 - val_loss: 0.3004\n",
      "Epoch 114/200\n",
      "1357/1357 [==============================] - 1s 642us/step - loss: 0.2996 - val_loss: 0.3002\n",
      "Epoch 115/200\n",
      "1357/1357 [==============================] - 1s 646us/step - loss: 0.2913 - val_loss: 0.2989\n",
      "Epoch 116/200\n",
      "1357/1357 [==============================] - 1s 638us/step - loss: 0.2973 - val_loss: 0.2927\n",
      "Epoch 117/200\n",
      "1357/1357 [==============================] - 1s 638us/step - loss: 0.2972 - val_loss: 0.3169\n",
      "Epoch 118/200\n",
      "1357/1357 [==============================] - 1s 639us/step - loss: 0.3063 - val_loss: 0.3166\n",
      "Epoch 119/200\n",
      "1357/1357 [==============================] - 1s 643us/step - loss: 0.2899 - val_loss: 0.2992\n",
      "Epoch 120/200\n",
      "1357/1357 [==============================] - 1s 639us/step - loss: 0.2926 - val_loss: 0.2970\n",
      "Epoch 121/200\n",
      "1357/1357 [==============================] - 1s 635us/step - loss: 0.2931 - val_loss: 0.3117\n",
      "Epoch 122/200\n",
      "1357/1357 [==============================] - 1s 633us/step - loss: 0.2936 - val_loss: 0.3012\n",
      "Epoch 123/200\n",
      "1357/1357 [==============================] - 1s 637us/step - loss: 0.2834 - val_loss: 0.2965\n",
      "Epoch 124/200\n",
      "1357/1357 [==============================] - 1s 642us/step - loss: 0.3029 - val_loss: 0.2982\n",
      "Epoch 125/200\n",
      "1357/1357 [==============================] - 1s 635us/step - loss: 0.2836 - val_loss: 0.3009\n",
      "Epoch 126/200\n",
      "1357/1357 [==============================] - 1s 642us/step - loss: 0.2820 - val_loss: 0.2999\n",
      "Epoch 127/200\n",
      "1357/1357 [==============================] - 1s 642us/step - loss: 0.2855 - val_loss: 0.2967\n",
      "Epoch 128/200\n",
      "1357/1357 [==============================] - 1s 634us/step - loss: 0.2952 - val_loss: 0.3021\n",
      "Epoch 129/200\n",
      "1357/1357 [==============================] - 1s 632us/step - loss: 0.2814 - val_loss: 0.3002\n",
      "Epoch 130/200\n",
      "1357/1357 [==============================] - 1s 697us/step - loss: 0.2904 - val_loss: 0.2983\n",
      "Epoch 131/200\n",
      "1357/1357 [==============================] - 1s 668us/step - loss: 0.2863 - val_loss: 0.3124\n",
      "Epoch 132/200\n",
      "1357/1357 [==============================] - 1s 691us/step - loss: 0.2728 - val_loss: 0.3044\n",
      "Epoch 00132: early stopping\n",
      "[CV]  activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2, total= 2.1min\n",
      "[CV] activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2 \n",
      "Train on 1357 samples, validate on 425 samples\n",
      "Epoch 1/200\n",
      "1357/1357 [==============================] - 2s 1ms/step - loss: 1.9784 - val_loss: 0.3877\n",
      "Epoch 2/200\n",
      "1357/1357 [==============================] - 1s 889us/step - loss: 1.2770 - val_loss: 0.6424\n",
      "Epoch 3/200\n",
      "1357/1357 [==============================] - 1s 866us/step - loss: 1.1086 - val_loss: 0.6605\n",
      "Epoch 4/200\n",
      "1357/1357 [==============================] - 1s 680us/step - loss: 1.0277 - val_loss: 0.8914\n",
      "Epoch 5/200\n",
      "1357/1357 [==============================] - 1s 666us/step - loss: 0.8917 - val_loss: 0.8452\n",
      "Epoch 6/200\n",
      "1357/1357 [==============================] - 1s 634us/step - loss: 0.8833 - val_loss: 0.6730\n",
      "Epoch 7/200\n",
      "1357/1357 [==============================] - 1s 615us/step - loss: 0.7579 - val_loss: 0.5976\n",
      "Epoch 8/200\n",
      "1357/1357 [==============================] - 1s 647us/step - loss: 0.6913 - val_loss: 0.6110\n",
      "Epoch 9/200\n",
      "1357/1357 [==============================] - 1s 607us/step - loss: 0.6678 - val_loss: 0.6329\n",
      "Epoch 10/200\n",
      "1357/1357 [==============================] - 1s 646us/step - loss: 0.6267 - val_loss: 0.5890\n",
      "Epoch 11/200\n",
      "1357/1357 [==============================] - 1s 644us/step - loss: 0.5955 - val_loss: 0.4787\n",
      "Epoch 12/200\n",
      "1357/1357 [==============================] - 1s 623us/step - loss: 0.5632 - val_loss: 0.5080\n",
      "Epoch 13/200\n",
      "1357/1357 [==============================] - 1s 620us/step - loss: 0.5544 - val_loss: 0.4979\n",
      "Epoch 14/200\n",
      "1357/1357 [==============================] - 1s 656us/step - loss: 0.5447 - val_loss: 0.4130\n",
      "Epoch 15/200\n",
      "1357/1357 [==============================] - 1s 612us/step - loss: 0.5131 - val_loss: 0.3752\n",
      "Epoch 16/200\n",
      "1357/1357 [==============================] - 1s 645us/step - loss: 0.5153 - val_loss: 0.3827\n",
      "Epoch 17/200\n",
      "1357/1357 [==============================] - 1s 649us/step - loss: 0.4756 - val_loss: 0.3849\n",
      "Epoch 18/200\n",
      "1357/1357 [==============================] - 1s 707us/step - loss: 0.4778 - val_loss: 0.3876\n",
      "Epoch 19/200\n",
      "1357/1357 [==============================] - 1s 655us/step - loss: 0.4423 - val_loss: 0.3636\n",
      "Epoch 20/200\n",
      "1357/1357 [==============================] - 1s 649us/step - loss: 0.4730 - val_loss: 0.3453\n",
      "Epoch 21/200\n",
      "1357/1357 [==============================] - 1s 643us/step - loss: 0.4538 - val_loss: 0.3180\n",
      "Epoch 22/200\n",
      "1357/1357 [==============================] - 1s 673us/step - loss: 0.4618 - val_loss: 0.3087\n",
      "Epoch 23/200\n",
      "1357/1357 [==============================] - 1s 668us/step - loss: 0.4419 - val_loss: 0.3237\n",
      "Epoch 24/200\n",
      "1357/1357 [==============================] - 1s 695us/step - loss: 0.4406 - val_loss: 0.3083\n",
      "Epoch 25/200\n",
      "1357/1357 [==============================] - 1s 660us/step - loss: 0.4439 - val_loss: 0.3132\n",
      "Epoch 26/200\n",
      "1357/1357 [==============================] - 1s 720us/step - loss: 0.4399 - val_loss: 0.3098\n",
      "Epoch 27/200\n",
      "1357/1357 [==============================] - 1s 734us/step - loss: 0.4392 - val_loss: 0.3094\n",
      "Epoch 28/200\n",
      "1357/1357 [==============================] - 1s 716us/step - loss: 0.4279 - val_loss: 0.3251\n",
      "Epoch 29/200\n",
      "1357/1357 [==============================] - 1s 707us/step - loss: 0.4379 - val_loss: 0.3179\n",
      "Epoch 30/200\n",
      "1357/1357 [==============================] - 1s 708us/step - loss: 0.4194 - val_loss: 0.3077\n",
      "Epoch 31/200\n",
      "1357/1357 [==============================] - 1s 668us/step - loss: 0.4219 - val_loss: 0.3081\n",
      "Epoch 32/200\n",
      "1357/1357 [==============================] - 1s 652us/step - loss: 0.4185 - val_loss: 0.3077\n",
      "Epoch 33/200\n",
      "1357/1357 [==============================] - 1s 683us/step - loss: 0.4371 - val_loss: 0.3092\n",
      "Epoch 34/200\n",
      "1357/1357 [==============================] - 1s 693us/step - loss: 0.4229 - val_loss: 0.3090\n",
      "Epoch 35/200\n",
      "1357/1357 [==============================] - 1s 612us/step - loss: 0.4341 - val_loss: 0.3089\n",
      "Epoch 36/200\n",
      "1357/1357 [==============================] - 1s 621us/step - loss: 0.4464 - val_loss: 0.3109\n",
      "Epoch 37/200\n",
      "1357/1357 [==============================] - 1s 614us/step - loss: 0.4097 - val_loss: 0.3105\n",
      "Epoch 38/200\n",
      "1357/1357 [==============================] - 1s 611us/step - loss: 0.4070 - val_loss: 0.3095\n",
      "Epoch 39/200\n",
      "1357/1357 [==============================] - 1s 612us/step - loss: 0.4397 - val_loss: 0.3078\n",
      "Epoch 40/200\n",
      "1357/1357 [==============================] - 1s 638us/step - loss: 0.4039 - val_loss: 0.3122\n",
      "Epoch 41/200\n",
      "1357/1357 [==============================] - 1s 634us/step - loss: 0.4253 - val_loss: 0.3110\n",
      "Epoch 42/200\n",
      "1357/1357 [==============================] - 1s 659us/step - loss: 0.4239 - val_loss: 0.3080\n",
      "Epoch 43/200\n",
      "1357/1357 [==============================] - 1s 619us/step - loss: 0.4142 - val_loss: 0.3076\n",
      "Epoch 44/200\n",
      "1357/1357 [==============================] - 1s 627us/step - loss: 0.3972 - val_loss: 0.3080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/200\n",
      "1357/1357 [==============================] - 1s 627us/step - loss: 0.4081 - val_loss: 0.3076\n",
      "Epoch 46/200\n",
      "1357/1357 [==============================] - 1s 661us/step - loss: 0.4074 - val_loss: 0.3084\n",
      "Epoch 47/200\n",
      "1357/1357 [==============================] - 1s 636us/step - loss: 0.4000 - val_loss: 0.3076\n",
      "Epoch 48/200\n",
      "1357/1357 [==============================] - 1s 650us/step - loss: 0.4063 - val_loss: 0.3080\n",
      "Epoch 49/200\n",
      "1357/1357 [==============================] - 1s 613us/step - loss: 0.4173 - val_loss: 0.3076\n",
      "Epoch 50/200\n",
      "1357/1357 [==============================] - 1s 619us/step - loss: 0.4219 - val_loss: 0.3180\n",
      "Epoch 51/200\n",
      "1357/1357 [==============================] - 1s 614us/step - loss: 0.3943 - val_loss: 0.3087\n",
      "Epoch 52/200\n",
      "1357/1357 [==============================] - 1s 678us/step - loss: 0.4075 - val_loss: 0.3120\n",
      "Epoch 53/200\n",
      "1357/1357 [==============================] - 1s 672us/step - loss: 0.4075 - val_loss: 0.3214\n",
      "Epoch 54/200\n",
      "1357/1357 [==============================] - 1s 718us/step - loss: 0.4006 - val_loss: 0.3078\n",
      "Epoch 55/200\n",
      "1357/1357 [==============================] - 1s 738us/step - loss: 0.3921 - val_loss: 0.3079\n",
      "Epoch 56/200\n",
      "1357/1357 [==============================] - 1s 736us/step - loss: 0.4024 - val_loss: 0.3088\n",
      "Epoch 57/200\n",
      "1357/1357 [==============================] - 1s 681us/step - loss: 0.3784 - val_loss: 0.3103\n",
      "Epoch 58/200\n",
      "1357/1357 [==============================] - 1s 655us/step - loss: 0.3963 - val_loss: 0.3288\n",
      "Epoch 59/200\n",
      "1357/1357 [==============================] - 1s 662us/step - loss: 0.3865 - val_loss: 0.3103\n",
      "Epoch 60/200\n",
      "1357/1357 [==============================] - 1s 699us/step - loss: 0.4084 - val_loss: 0.3160\n",
      "Epoch 61/200\n",
      "1357/1357 [==============================] - 1s 647us/step - loss: 0.4172 - val_loss: 0.3129\n",
      "Epoch 62/200\n",
      "1357/1357 [==============================] - 1s 631us/step - loss: 0.4005 - val_loss: 0.3076\n",
      "Epoch 63/200\n",
      "1357/1357 [==============================] - 1s 622us/step - loss: 0.3918 - val_loss: 0.3080\n",
      "Epoch 64/200\n",
      "1357/1357 [==============================] - 1s 653us/step - loss: 0.3974 - val_loss: 0.3101\n",
      "Epoch 65/200\n",
      "1357/1357 [==============================] - 1s 665us/step - loss: 0.3942 - val_loss: 0.3091\n",
      "Epoch 66/200\n",
      "1357/1357 [==============================] - 1s 616us/step - loss: 0.3899 - val_loss: 0.3096\n",
      "Epoch 67/200\n",
      "1357/1357 [==============================] - 1s 648us/step - loss: 0.4004 - val_loss: 0.3076\n",
      "Epoch 68/200\n",
      "1357/1357 [==============================] - 1s 617us/step - loss: 0.3846 - val_loss: 0.3243\n",
      "Epoch 00068: early stopping\n",
      "[CV]  activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2, total= 1.1min\n",
      "[CV] activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2 \n",
      "Train on 1357 samples, validate on 425 samples\n",
      "Epoch 1/200\n",
      "1357/1357 [==============================] - 1s 1ms/step - loss: 1.8474 - val_loss: 0.4436\n",
      "Epoch 2/200\n",
      "1357/1357 [==============================] - 1s 663us/step - loss: 1.3175 - val_loss: 0.6886\n",
      "Epoch 3/200\n",
      "1357/1357 [==============================] - 1s 643us/step - loss: 1.1581 - val_loss: 0.8829\n",
      "Epoch 4/200\n",
      "1357/1357 [==============================] - 1s 627us/step - loss: 0.9992 - val_loss: 0.7190\n",
      "Epoch 5/200\n",
      "1357/1357 [==============================] - 1s 626us/step - loss: 0.9578 - val_loss: 0.8873\n",
      "Epoch 6/200\n",
      "1357/1357 [==============================] - 1s 621us/step - loss: 0.8123 - val_loss: 0.7772\n",
      "Epoch 7/200\n",
      "1357/1357 [==============================] - 1s 637us/step - loss: 0.7556 - val_loss: 0.7680\n",
      "Epoch 8/200\n",
      "1357/1357 [==============================] - 1s 607us/step - loss: 0.6996 - val_loss: 0.7550\n",
      "Epoch 9/200\n",
      "1357/1357 [==============================] - 1s 608us/step - loss: 0.6706 - val_loss: 0.5930\n",
      "Epoch 10/200\n",
      "1357/1357 [==============================] - 1s 600us/step - loss: 0.6055 - val_loss: 0.5628\n",
      "Epoch 11/200\n",
      "1357/1357 [==============================] - 1s 621us/step - loss: 0.5749 - val_loss: 0.5617\n",
      "Epoch 12/200\n",
      "1357/1357 [==============================] - 1s 614us/step - loss: 0.5775 - val_loss: 0.4220\n",
      "Epoch 13/200\n",
      "1357/1357 [==============================] - 1s 634us/step - loss: 0.5210 - val_loss: 0.4785\n",
      "Epoch 14/200\n",
      "1357/1357 [==============================] - 1s 633us/step - loss: 0.5100 - val_loss: 0.4461\n",
      "Epoch 15/200\n",
      "1357/1357 [==============================] - 1s 613us/step - loss: 0.4981 - val_loss: 0.3880\n",
      "Epoch 16/200\n",
      "1357/1357 [==============================] - 1s 610us/step - loss: 0.4973 - val_loss: 0.3339\n",
      "Epoch 17/200\n",
      "1357/1357 [==============================] - 1s 620us/step - loss: 0.5086 - val_loss: 0.3888\n",
      "Epoch 18/200\n",
      "1357/1357 [==============================] - 1s 721us/step - loss: 0.4672 - val_loss: 0.3519\n",
      "Epoch 19/200\n",
      "1357/1357 [==============================] - 1s 685us/step - loss: 0.4572 - val_loss: 0.3262\n",
      "Epoch 20/200\n",
      "1357/1357 [==============================] - 1s 696us/step - loss: 0.4456 - val_loss: 0.3407\n",
      "Epoch 21/200\n",
      "1357/1357 [==============================] - 1s 667us/step - loss: 0.4560 - val_loss: 0.3310\n",
      "Epoch 22/200\n",
      "1357/1357 [==============================] - 1s 657us/step - loss: 0.4614 - val_loss: 0.3214\n",
      "Epoch 23/200\n",
      "1357/1357 [==============================] - 1s 662us/step - loss: 0.4320 - val_loss: 0.3182\n",
      "Epoch 24/200\n",
      "1357/1357 [==============================] - 1s 714us/step - loss: 0.4375 - val_loss: 0.3206\n",
      "Epoch 25/200\n",
      "1357/1357 [==============================] - 1s 617us/step - loss: 0.4464 - val_loss: 0.3169\n",
      "Epoch 26/200\n",
      "1357/1357 [==============================] - 1s 629us/step - loss: 0.4417 - val_loss: 0.3090\n",
      "Epoch 27/200\n",
      "1357/1357 [==============================] - 1s 604us/step - loss: 0.4445 - val_loss: 0.3244\n",
      "Epoch 28/200\n",
      "1357/1357 [==============================] - 1s 603us/step - loss: 0.4196 - val_loss: 0.3194\n",
      "Epoch 29/200\n",
      "1357/1357 [==============================] - 1s 610us/step - loss: 0.4298 - val_loss: 0.3176\n",
      "Epoch 30/200\n",
      "1357/1357 [==============================] - 1s 635us/step - loss: 0.4234 - val_loss: 0.3080\n",
      "Epoch 31/200\n",
      "1357/1357 [==============================] - 1s 589us/step - loss: 0.4166 - val_loss: 0.3144\n",
      "Epoch 32/200\n",
      "1357/1357 [==============================] - 1s 579us/step - loss: 0.4265 - val_loss: 0.3092\n",
      "Epoch 33/200\n",
      "1357/1357 [==============================] - 1s 599us/step - loss: 0.4311 - val_loss: 0.3106\n",
      "Epoch 34/200\n",
      "1357/1357 [==============================] - 1s 664us/step - loss: 0.4077 - val_loss: 0.3084\n",
      "Epoch 35/200\n",
      "1357/1357 [==============================] - 1s 641us/step - loss: 0.4242 - val_loss: 0.3105\n",
      "Epoch 36/200\n",
      "1357/1357 [==============================] - 1s 632us/step - loss: 0.4353 - val_loss: 0.3107\n",
      "Epoch 37/200\n",
      "1357/1357 [==============================] - 1s 812us/step - loss: 0.4387 - val_loss: 0.3077\n",
      "Epoch 38/200\n",
      "1357/1357 [==============================] - 1s 651us/step - loss: 0.4043 - val_loss: 0.3086\n",
      "Epoch 39/200\n",
      "1357/1357 [==============================] - 1s 655us/step - loss: 0.4099 - val_loss: 0.3086\n",
      "Epoch 40/200\n",
      "1357/1357 [==============================] - 1s 656us/step - loss: 0.4211 - val_loss: 0.3115\n",
      "Epoch 41/200\n",
      "1357/1357 [==============================] - 1s 657us/step - loss: 0.4139 - val_loss: 0.3128\n",
      "Epoch 42/200\n",
      "1357/1357 [==============================] - 1s 673us/step - loss: 0.4152 - val_loss: 0.3076\n",
      "Epoch 43/200\n",
      "1357/1357 [==============================] - 1s 691us/step - loss: 0.4069 - val_loss: 0.3159\n",
      "Epoch 44/200\n",
      "1357/1357 [==============================] - 1s 699us/step - loss: 0.4182 - val_loss: 0.3115\n",
      "Epoch 45/200\n",
      "1357/1357 [==============================] - 1s 656us/step - loss: 0.4161 - val_loss: 0.3093\n",
      "Epoch 46/200\n",
      "1357/1357 [==============================] - 1s 779us/step - loss: 0.3959 - val_loss: 0.3118\n",
      "Epoch 47/200\n",
      "1357/1357 [==============================] - 1s 618us/step - loss: 0.4025 - val_loss: 0.3228\n",
      "Epoch 48/200\n",
      "1357/1357 [==============================] - 1s 619us/step - loss: 0.3977 - val_loss: 0.3081\n",
      "Epoch 49/200\n",
      "1357/1357 [==============================] - 1s 627us/step - loss: 0.4042 - val_loss: 0.3078\n",
      "Epoch 50/200\n",
      "1357/1357 [==============================] - 1s 636us/step - loss: 0.3898 - val_loss: 0.3087\n",
      "Epoch 51/200\n",
      "1357/1357 [==============================] - 1s 614us/step - loss: 0.3921 - val_loss: 0.3083\n",
      "Epoch 52/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1357/1357 [==============================] - 1s 639us/step - loss: 0.4198 - val_loss: 0.3086\n",
      "Epoch 53/200\n",
      "1357/1357 [==============================] - 1s 649us/step - loss: 0.3902 - val_loss: 0.3088\n",
      "Epoch 54/200\n",
      "1357/1357 [==============================] - 1s 632us/step - loss: 0.4021 - val_loss: 0.3145\n",
      "Epoch 55/200\n",
      "1357/1357 [==============================] - 1s 616us/step - loss: 0.3941 - val_loss: 0.3291\n",
      "Epoch 56/200\n",
      "1357/1357 [==============================] - 1s 655us/step - loss: 0.4130 - val_loss: 0.3076\n",
      "Epoch 57/200\n",
      "1357/1357 [==============================] - 1s 619us/step - loss: 0.3905 - val_loss: 0.3106\n",
      "Epoch 58/200\n",
      "1357/1357 [==============================] - 1s 610us/step - loss: 0.3894 - val_loss: 0.3099\n",
      "Epoch 59/200\n",
      "1357/1357 [==============================] - 1s 653us/step - loss: 0.4033 - val_loss: 0.3077\n",
      "Epoch 60/200\n",
      "1357/1357 [==============================] - 1s 648us/step - loss: 0.4101 - val_loss: 0.3116\n",
      "Epoch 61/200\n",
      "1357/1357 [==============================] - 1s 595us/step - loss: 0.3795 - val_loss: 0.3113\n",
      "Epoch 62/200\n",
      "1357/1357 [==============================] - 1s 594us/step - loss: 0.3904 - val_loss: 0.3129\n",
      "Epoch 00062: early stopping\n",
      "[CV]  activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2, total=  56.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1696 samples, validate on 425 samples\n",
      "Epoch 1/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 1.8554 - val_loss: 0.3946\n",
      "Epoch 2/200\n",
      "1696/1696 [==============================] - 2s 916us/step - loss: 1.3672 - val_loss: 0.4912\n",
      "Epoch 3/200\n",
      "1696/1696 [==============================] - 2s 915us/step - loss: 1.1532 - val_loss: 0.5673\n",
      "Epoch 4/200\n",
      "1696/1696 [==============================] - 2s 886us/step - loss: 0.9767 - val_loss: 0.8649\n",
      "Epoch 5/200\n",
      "1696/1696 [==============================] - 1s 832us/step - loss: 0.8672 - val_loss: 0.7874\n",
      "Epoch 6/200\n",
      "1696/1696 [==============================] - 2s 962us/step - loss: 0.7175 - val_loss: 0.8603\n",
      "Epoch 7/200\n",
      "1696/1696 [==============================] - 1s 874us/step - loss: 0.6594 - val_loss: 0.6190\n",
      "Epoch 8/200\n",
      "1696/1696 [==============================] - 2s 928us/step - loss: 0.6650 - val_loss: 0.6666\n",
      "Epoch 9/200\n",
      "1696/1696 [==============================] - 2s 945us/step - loss: 0.6261 - val_loss: 0.5357\n",
      "Epoch 10/200\n",
      "1696/1696 [==============================] - 1s 882us/step - loss: 0.5471 - val_loss: 0.5379\n",
      "Epoch 11/200\n",
      "1696/1696 [==============================] - 2s 940us/step - loss: 0.5649 - val_loss: 0.5245\n",
      "Epoch 12/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.5108 - val_loss: 0.4377\n",
      "Epoch 13/200\n",
      "1696/1696 [==============================] - 1s 857us/step - loss: 0.4974 - val_loss: 0.4103\n",
      "Epoch 14/200\n",
      "1696/1696 [==============================] - 1s 876us/step - loss: 0.4590 - val_loss: 0.4073\n",
      "Epoch 15/200\n",
      "1696/1696 [==============================] - 1s 825us/step - loss: 0.4525 - val_loss: 0.3377\n",
      "Epoch 16/200\n",
      "1696/1696 [==============================] - 2s 910us/step - loss: 0.4579 - val_loss: 0.3474\n",
      "Epoch 17/200\n",
      "1696/1696 [==============================] - 2s 901us/step - loss: 0.4226 - val_loss: 0.3381\n",
      "Epoch 18/200\n",
      "1696/1696 [==============================] - 1s 878us/step - loss: 0.4192 - val_loss: 0.3325\n",
      "Epoch 19/200\n",
      "1696/1696 [==============================] - 1s 849us/step - loss: 0.4352 - val_loss: 0.3192\n",
      "Epoch 20/200\n",
      "1696/1696 [==============================] - 1s 839us/step - loss: 0.4508 - val_loss: 0.2991\n",
      "Epoch 21/200\n",
      "1696/1696 [==============================] - 1s 858us/step - loss: 0.4255 - val_loss: 0.3171\n",
      "Epoch 22/200\n",
      "1696/1696 [==============================] - 2s 981us/step - loss: 0.4367 - val_loss: 0.3185\n",
      "Epoch 23/200\n",
      "1696/1696 [==============================] - 2s 890us/step - loss: 0.4217 - val_loss: 0.3116\n",
      "Epoch 24/200\n",
      "1696/1696 [==============================] - 2s 962us/step - loss: 0.4179 - val_loss: 0.3054\n",
      "Epoch 25/200\n",
      "1696/1696 [==============================] - 2s 927us/step - loss: 0.3995 - val_loss: 0.3013\n",
      "Epoch 26/200\n",
      "1696/1696 [==============================] - 2s 969us/step - loss: 0.4169 - val_loss: 0.3326\n",
      "Epoch 27/200\n",
      "1696/1696 [==============================] - 2s 938us/step - loss: 0.3956 - val_loss: 0.3083\n",
      "Epoch 28/200\n",
      "1696/1696 [==============================] - 2s 971us/step - loss: 0.4136 - val_loss: 0.3004\n",
      "Epoch 29/200\n",
      "1696/1696 [==============================] - 2s 955us/step - loss: 0.4150 - val_loss: 0.2967\n",
      "Epoch 30/200\n",
      "1696/1696 [==============================] - 2s 931us/step - loss: 0.4147 - val_loss: 0.3108\n",
      "Epoch 31/200\n",
      "1696/1696 [==============================] - 2s 970us/step - loss: 0.3902 - val_loss: 0.3110\n",
      "Epoch 32/200\n",
      "1696/1696 [==============================] - 2s 951us/step - loss: 0.4066 - val_loss: 0.3079\n",
      "Epoch 33/200\n",
      "1696/1696 [==============================] - 1s 863us/step - loss: 0.4151 - val_loss: 0.3082\n",
      "Epoch 34/200\n",
      "1696/1696 [==============================] - 2s 891us/step - loss: 0.4109 - val_loss: 0.3110\n",
      "Epoch 35/200\n",
      "1696/1696 [==============================] - 1s 856us/step - loss: 0.4116 - val_loss: 0.3124\n",
      "Epoch 36/200\n",
      "1696/1696 [==============================] - 1s 870us/step - loss: 0.4066 - val_loss: 0.3370\n",
      "Epoch 37/200\n",
      "1696/1696 [==============================] - 1s 861us/step - loss: 0.4376 - val_loss: 0.3088\n",
      "Epoch 38/200\n",
      "1696/1696 [==============================] - 2s 972us/step - loss: 0.3931 - val_loss: 0.3098\n",
      "Epoch 39/200\n",
      "1696/1696 [==============================] - 2s 905us/step - loss: 0.4056 - val_loss: 0.3145\n",
      "Epoch 40/200\n",
      "1696/1696 [==============================] - 2s 908us/step - loss: 0.4030 - val_loss: 0.3116\n",
      "Epoch 41/200\n",
      "1696/1696 [==============================] - 1s 849us/step - loss: 0.4046 - val_loss: 0.3079\n",
      "Epoch 42/200\n",
      "1696/1696 [==============================] - 1s 858us/step - loss: 0.4117 - val_loss: 0.3080\n",
      "Epoch 43/200\n",
      "1696/1696 [==============================] - 1s 837us/step - loss: 0.4122 - val_loss: 0.3082\n",
      "Epoch 44/200\n",
      "1696/1696 [==============================] - 1s 825us/step - loss: 0.4163 - val_loss: 0.3135\n",
      "Epoch 45/200\n",
      "1696/1696 [==============================] - 1s 848us/step - loss: 0.4011 - val_loss: 0.3086\n",
      "Epoch 46/200\n",
      "1696/1696 [==============================] - 2s 885us/step - loss: 0.3991 - val_loss: 0.3092\n",
      "Epoch 47/200\n",
      "1696/1696 [==============================] - 1s 816us/step - loss: 0.3842 - val_loss: 0.3101\n",
      "Epoch 48/200\n",
      "1696/1696 [==============================] - 1s 826us/step - loss: 0.4094 - val_loss: 0.3084\n",
      "Epoch 49/200\n",
      "1696/1696 [==============================] - 1s 840us/step - loss: 0.3942 - val_loss: 0.3082\n",
      "Epoch 50/200\n",
      "1696/1696 [==============================] - 1s 873us/step - loss: 0.4004 - val_loss: 0.3076\n",
      "Epoch 51/200\n",
      "1696/1696 [==============================] - 1s 847us/step - loss: 0.4000 - val_loss: 0.3091\n",
      "Epoch 52/200\n",
      "1696/1696 [==============================] - 1s 848us/step - loss: 0.3932 - val_loss: 0.3076\n",
      "Epoch 53/200\n",
      "1696/1696 [==============================] - 1s 882us/step - loss: 0.4013 - val_loss: 0.3076\n",
      "Epoch 54/200\n",
      "1696/1696 [==============================] - 1s 874us/step - loss: 0.3875 - val_loss: 0.3110\n",
      "Epoch 00054: early stopping\n",
      "The parameters of the best model are: \n",
      "{'activation_function': 'tanh', 'hidden_layer_size': 64, 'num_hidden_layers': 2}\n",
      "CPU times: user 6min 54s, sys: 23.6 s, total: 7min 17s\n",
      "Wall time: 7min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "classifier = KerasRegressor(make_model, batch_size=32, epochs=200)\n",
    "\n",
    "# params = [{'num_hidden_layers': [0],\n",
    "#           'hidden_layer_size': [0],\n",
    "#           'activation_function': ['linear', 'sigmoid', 'relu', 'tanh']},\n",
    "#           {'num_hidden_layers': [1,2,3],\n",
    "#           'hidden_layer_size': [64, 128, 256],\n",
    "#           'activation_function': ['linear', 'sigmoid', 'relu', 'tanh']}]\n",
    "\n",
    "params = [{'num_hidden_layers': [2],\n",
    "          'hidden_layer_size': [64],\n",
    "          'activation_function': ['tanh']}]\n",
    "\n",
    "\n",
    "grid = GridSearchCV(classifier,\n",
    "                         param_grid=params,\n",
    "                         scoring='neg_mean_squared_error', #sklearn optimizing by maximizing negative MSE\n",
    "                         n_jobs=1,\n",
    "                         verbose=2,\n",
    "                         cv=KFold(n_splits=5, shuffle=True, random_state=42),# Number of folds for CV\n",
    "                         return_train_score=True,\n",
    "                         error_score='raise'\n",
    "                   )\n",
    "\n",
    "grid.fit(np.array(X_train), np.array(y_train),\n",
    "        **{'callbacks': [EarlyStopping(monitor='val_loss', \n",
    "                                                                 min_delta=0.001, \n",
    "                                                                 patience=25, \n",
    "                                                                 mode='min',\n",
    "                                                                 verbose=2)],\n",
    "                                    'validation_data': (np.array(X_test), np.array(y_test))\n",
    "                                    })\n",
    "\n",
    "print('The parameters of the best model are: ')\n",
    "print(grid.best_params_)\n",
    "\n",
    "best_model = grid.best_estimator_ #scikit-wrapped best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation_function': 'tanh',\n",
       " 'hidden_layer_size': 64,\n",
       " 'num_hidden_layers': 2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores can the be extracted to view performance of every model on every fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n",
       "       'param_activation_function', 'param_hidden_layer_size',\n",
       "       'param_num_hidden_layers', 'params', 'split0_test_score',\n",
       "       'split1_test_score', 'split2_test_score', 'split3_test_score',\n",
       "       'split4_test_score', 'mean_test_score', 'std_test_score',\n",
       "       'rank_test_score', 'split0_train_score', 'split1_train_score',\n",
       "       'split2_train_score', 'split3_train_score', 'split4_train_score',\n",
       "       'mean_train_score', 'std_train_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation_function</th>\n",
       "      <th>param_hidden_layer_size</th>\n",
       "      <th>param_num_hidden_layers</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.772343</td>\n",
       "      <td>28.543953</td>\n",
       "      <td>0.110753</td>\n",
       "      <td>0.021249</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>{'activation_function': 'tanh', 'hidden_layer_...</td>\n",
       "      <td>-0.331345</td>\n",
       "      <td>-0.303178</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314166</td>\n",
       "      <td>0.013047</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.314232</td>\n",
       "      <td>-0.321803</td>\n",
       "      <td>-0.174878</td>\n",
       "      <td>-0.321403</td>\n",
       "      <td>-0.31434</td>\n",
       "      <td>-0.289331</td>\n",
       "      <td>0.05732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      69.772343     28.543953         0.110753        0.021249   \n",
       "\n",
       "  param_activation_function param_hidden_layer_size param_num_hidden_layers  \\\n",
       "0                      tanh                      64                       2   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'activation_function': 'tanh', 'hidden_layer_...          -0.331345   \n",
       "\n",
       "   split1_test_score       ...         mean_test_score  std_test_score  \\\n",
       "0          -0.303178       ...               -0.314166        0.013047   \n",
       "\n",
       "   rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0                1           -0.314232           -0.321803   \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0           -0.174878           -0.321403            -0.31434   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "0         -0.289331          0.05732  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(['rank_test_score'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing models (based on the grid-search) can then be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_num_hidden_layers</th>\n",
       "      <th>param_hidden_layer_size</th>\n",
       "      <th>param_activation_function</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>64</th>\n",
       "      <th>tanh</th>\n",
       "      <td>69.772343</td>\n",
       "      <td>28.543953</td>\n",
       "      <td>0.110753</td>\n",
       "      <td>0.021249</td>\n",
       "      <td>-0.331345</td>\n",
       "      <td>-0.303178</td>\n",
       "      <td>-0.295473</td>\n",
       "      <td>-0.318396</td>\n",
       "      <td>-0.322387</td>\n",
       "      <td>-0.314166</td>\n",
       "      <td>0.013047</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.314232</td>\n",
       "      <td>-0.321803</td>\n",
       "      <td>-0.174878</td>\n",
       "      <td>-0.321403</td>\n",
       "      <td>-0.31434</td>\n",
       "      <td>-0.289331</td>\n",
       "      <td>0.05732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           mean_fit_time  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                  \n",
       "2                       64                      tanh                           69.772343   \n",
       "\n",
       "                                                                           std_fit_time  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                 \n",
       "2                       64                      tanh                          28.543953   \n",
       "\n",
       "                                                                           mean_score_time  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                    \n",
       "2                       64                      tanh                              0.110753   \n",
       "\n",
       "                                                                           std_score_time  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                   \n",
       "2                       64                      tanh                             0.021249   \n",
       "\n",
       "                                                                           split0_test_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                      \n",
       "2                       64                      tanh                               -0.331345   \n",
       "\n",
       "                                                                           split1_test_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                      \n",
       "2                       64                      tanh                               -0.303178   \n",
       "\n",
       "                                                                           split2_test_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                      \n",
       "2                       64                      tanh                               -0.295473   \n",
       "\n",
       "                                                                           split3_test_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                      \n",
       "2                       64                      tanh                               -0.318396   \n",
       "\n",
       "                                                                           split4_test_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                      \n",
       "2                       64                      tanh                               -0.322387   \n",
       "\n",
       "                                                                           mean_test_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                    \n",
       "2                       64                      tanh                             -0.314166   \n",
       "\n",
       "                                                                           std_test_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                   \n",
       "2                       64                      tanh                             0.013047   \n",
       "\n",
       "                                                                           rank_test_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                    \n",
       "2                       64                      tanh                                     1   \n",
       "\n",
       "                                                                           split0_train_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                       \n",
       "2                       64                      tanh                                -0.314232   \n",
       "\n",
       "                                                                           split1_train_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                       \n",
       "2                       64                      tanh                                -0.321803   \n",
       "\n",
       "                                                                           split2_train_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                       \n",
       "2                       64                      tanh                                -0.174878   \n",
       "\n",
       "                                                                           split3_train_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                       \n",
       "2                       64                      tanh                                -0.321403   \n",
       "\n",
       "                                                                           split4_train_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                       \n",
       "2                       64                      tanh                                 -0.31434   \n",
       "\n",
       "                                                                           mean_train_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                     \n",
       "2                       64                      tanh                              -0.289331   \n",
       "\n",
       "                                                                           std_train_score  \n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                   \n",
       "2                       64                      tanh                               0.05732  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = results.groupby(['param_num_hidden_layers','param_hidden_layer_size','param_activation_function'])\n",
    "grouped = grouped.mean()\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.to_csv('../../output/model_params_and_results.csv') # Save csv so it can be used as a table in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find the top 5 best performing models and look more closely at their performance. \n",
    "\n",
    "Note that they all have very similar scores, all of which are far better than we would expect (none of the leaderboard scores are better than 0.38). This suggests that there is some overfitting in these models, despite the use of dropout, cross-validation, and early stopping using validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_num_hidden_layers</th>\n",
       "      <th>param_hidden_layer_size</th>\n",
       "      <th>param_activation_function</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>64</th>\n",
       "      <th>tanh</th>\n",
       "      <td>69.772343</td>\n",
       "      <td>28.543953</td>\n",
       "      <td>0.110753</td>\n",
       "      <td>0.021249</td>\n",
       "      <td>-0.331345</td>\n",
       "      <td>-0.303178</td>\n",
       "      <td>-0.295473</td>\n",
       "      <td>-0.318396</td>\n",
       "      <td>-0.322387</td>\n",
       "      <td>-0.314166</td>\n",
       "      <td>0.013047</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.314232</td>\n",
       "      <td>-0.321803</td>\n",
       "      <td>-0.174878</td>\n",
       "      <td>-0.321403</td>\n",
       "      <td>-0.31434</td>\n",
       "      <td>-0.289331</td>\n",
       "      <td>0.05732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           mean_fit_time  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                  \n",
       "2                       64                      tanh                           69.772343   \n",
       "\n",
       "                                                                           std_fit_time  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                 \n",
       "2                       64                      tanh                          28.543953   \n",
       "\n",
       "                                                                           mean_score_time  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                    \n",
       "2                       64                      tanh                              0.110753   \n",
       "\n",
       "                                                                           std_score_time  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                   \n",
       "2                       64                      tanh                             0.021249   \n",
       "\n",
       "                                                                           split0_test_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                      \n",
       "2                       64                      tanh                               -0.331345   \n",
       "\n",
       "                                                                           split1_test_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                      \n",
       "2                       64                      tanh                               -0.303178   \n",
       "\n",
       "                                                                           split2_test_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                      \n",
       "2                       64                      tanh                               -0.295473   \n",
       "\n",
       "                                                                           split3_test_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                      \n",
       "2                       64                      tanh                               -0.318396   \n",
       "\n",
       "                                                                           split4_test_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                      \n",
       "2                       64                      tanh                               -0.322387   \n",
       "\n",
       "                                                                           mean_test_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                    \n",
       "2                       64                      tanh                             -0.314166   \n",
       "\n",
       "                                                                           std_test_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                   \n",
       "2                       64                      tanh                             0.013047   \n",
       "\n",
       "                                                                           rank_test_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                    \n",
       "2                       64                      tanh                                     1   \n",
       "\n",
       "                                                                           split0_train_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                       \n",
       "2                       64                      tanh                                -0.314232   \n",
       "\n",
       "                                                                           split1_train_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                       \n",
       "2                       64                      tanh                                -0.321803   \n",
       "\n",
       "                                                                           split2_train_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                       \n",
       "2                       64                      tanh                                -0.174878   \n",
       "\n",
       "                                                                           split3_train_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                       \n",
       "2                       64                      tanh                                -0.321403   \n",
       "\n",
       "                                                                           split4_train_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                       \n",
       "2                       64                      tanh                                 -0.31434   \n",
       "\n",
       "                                                                           mean_train_score  \\\n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                     \n",
       "2                       64                      tanh                              -0.289331   \n",
       "\n",
       "                                                                           std_train_score  \n",
       "param_num_hidden_layers param_hidden_layer_size param_activation_function                   \n",
       "2                       64                      tanh                               0.05732  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = grouped.sort_values(['rank_test_score'],ascending=True).head(5)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_models = []\n",
    "for _, r in best.iterrows():\n",
    "    p = {'num_hidden_layers': [_[0]],\n",
    "          'hidden_layer_size': [_[1]],\n",
    "          'activation_function': [_[2]]}\n",
    "    final_models.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to retrain the 5 best models and assess the out-of-sample performance of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can use the same gridsearch with a single set of parameters to iterate over the models and get predictions. The only changes I'm making are the inclusion of additional callback parameters to store the results and a slight decrease in the `patience` parameter to help prevent overfitting.\n",
    "\n",
    "This time I also store each estimator, its predictions, and its history in a dictionary.\n",
    "\n",
    "***Note: It takes at least 2 hours to run these models.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seeds again to ensure reproducibility\n",
    "np.random.seed(67891)\n",
    "random.seed(54321)\n",
    "tf.set_random_seed(56789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1356 samples, validate on 425 samples\n",
      "Epoch 1/200\n",
      "1356/1356 [==============================] - 2s 1ms/step - loss: 1.7378 - val_loss: 0.4423\n",
      "Epoch 2/200\n",
      "1356/1356 [==============================] - 1s 558us/step - loss: 1.3455 - val_loss: 1.1612\n",
      "Epoch 3/200\n",
      "1356/1356 [==============================] - 1s 549us/step - loss: 1.2217 - val_loss: 0.6494\n",
      "Epoch 4/200\n",
      "1356/1356 [==============================] - 1s 562us/step - loss: 1.0463 - val_loss: 1.0708\n",
      "Epoch 5/200\n",
      "1356/1356 [==============================] - 1s 564us/step - loss: 0.9283 - val_loss: 0.8284\n",
      "Epoch 6/200\n",
      "1356/1356 [==============================] - 1s 573us/step - loss: 0.7949 - val_loss: 0.7453\n",
      "Epoch 7/200\n",
      "1356/1356 [==============================] - 1s 613us/step - loss: 0.7159 - val_loss: 0.6420\n",
      "Epoch 8/200\n",
      "1356/1356 [==============================] - 1s 555us/step - loss: 0.7050 - val_loss: 0.6250\n",
      "Epoch 9/200\n",
      "1356/1356 [==============================] - 1s 548us/step - loss: 0.6242 - val_loss: 0.5742\n",
      "Epoch 10/200\n",
      "1356/1356 [==============================] - 1s 538us/step - loss: 0.6355 - val_loss: 0.6251\n",
      "Epoch 11/200\n",
      "1356/1356 [==============================] - 1s 561us/step - loss: 0.5944 - val_loss: 0.4643\n",
      "Epoch 12/200\n",
      "1356/1356 [==============================] - 1s 545us/step - loss: 0.5764 - val_loss: 0.4912\n",
      "Epoch 13/200\n",
      "1356/1356 [==============================] - 1s 570us/step - loss: 0.5431 - val_loss: 0.4359\n",
      "Epoch 14/200\n",
      "1356/1356 [==============================] - 1s 594us/step - loss: 0.5249 - val_loss: 0.4841\n",
      "Epoch 15/200\n",
      "1356/1356 [==============================] - 1s 600us/step - loss: 0.5038 - val_loss: 0.4695\n",
      "Epoch 16/200\n",
      "1356/1356 [==============================] - 1s 589us/step - loss: 0.4984 - val_loss: 0.3611\n",
      "Epoch 17/200\n",
      "1356/1356 [==============================] - 1s 552us/step - loss: 0.4583 - val_loss: 0.3823\n",
      "Epoch 18/200\n",
      "1356/1356 [==============================] - 1s 768us/step - loss: 0.4732 - val_loss: 0.3636\n",
      "Epoch 19/200\n",
      "1356/1356 [==============================] - 1s 711us/step - loss: 0.4604 - val_loss: 0.3204\n",
      "Epoch 20/200\n",
      "1356/1356 [==============================] - 1s 721us/step - loss: 0.4563 - val_loss: 0.3299\n",
      "Epoch 21/200\n",
      "1356/1356 [==============================] - 1s 711us/step - loss: 0.4368 - val_loss: 0.3315\n",
      "Epoch 22/200\n",
      "1356/1356 [==============================] - 1s 686us/step - loss: 0.4491 - val_loss: 0.3581\n",
      "Epoch 23/200\n",
      "1356/1356 [==============================] - 1s 650us/step - loss: 0.4516 - val_loss: 0.3217\n",
      "Epoch 24/200\n",
      "1356/1356 [==============================] - 1s 701us/step - loss: 0.4399 - val_loss: 0.3214\n",
      "Epoch 25/200\n",
      "1356/1356 [==============================] - 1s 683us/step - loss: 0.4198 - val_loss: 0.3124\n",
      "Epoch 26/200\n",
      "1356/1356 [==============================] - 1s 814us/step - loss: 0.4249 - val_loss: 0.3220\n",
      "Epoch 27/200\n",
      "1356/1356 [==============================] - 1s 774us/step - loss: 0.4208 - val_loss: 0.3076\n",
      "Epoch 28/200\n",
      "1356/1356 [==============================] - 1s 711us/step - loss: 0.4415 - val_loss: 0.3224\n",
      "Epoch 29/200\n",
      "1356/1356 [==============================] - 1s 673us/step - loss: 0.4387 - val_loss: 0.3270\n",
      "Epoch 30/200\n",
      "1356/1356 [==============================] - 1s 660us/step - loss: 0.4354 - val_loss: 0.3102\n",
      "Epoch 31/200\n",
      "1356/1356 [==============================] - 1s 675us/step - loss: 0.4268 - val_loss: 0.3214\n",
      "Epoch 32/200\n",
      "1356/1356 [==============================] - 1s 666us/step - loss: 0.4190 - val_loss: 0.3084\n",
      "Epoch 33/200\n",
      "1356/1356 [==============================] - 1s 670us/step - loss: 0.4230 - val_loss: 0.3076\n",
      "Epoch 34/200\n",
      "1356/1356 [==============================] - 1s 657us/step - loss: 0.4133 - val_loss: 0.3091\n",
      "Epoch 35/200\n",
      "1356/1356 [==============================] - 1s 749us/step - loss: 0.4193 - val_loss: 0.3078\n",
      "Epoch 36/200\n",
      "1356/1356 [==============================] - 1s 672us/step - loss: 0.4210 - val_loss: 0.3154\n",
      "Epoch 37/200\n",
      "1356/1356 [==============================] - 1s 637us/step - loss: 0.4255 - val_loss: 0.3120\n",
      "Epoch 38/200\n",
      "1356/1356 [==============================] - 1s 649us/step - loss: 0.4036 - val_loss: 0.3076\n",
      "Epoch 39/200\n",
      "1356/1356 [==============================] - 1s 739us/step - loss: 0.4055 - val_loss: 0.3082\n",
      "Epoch 40/200\n",
      "1356/1356 [==============================] - 1s 655us/step - loss: 0.4084 - val_loss: 0.3086\n",
      "Epoch 41/200\n",
      "1356/1356 [==============================] - 1s 634us/step - loss: 0.4199 - val_loss: 0.3076\n",
      "Epoch 42/200\n",
      "1356/1356 [==============================] - 1s 645us/step - loss: 0.4179 - val_loss: 0.3131\n",
      "Epoch 43/200\n",
      "1356/1356 [==============================] - 1s 638us/step - loss: 0.4096 - val_loss: 0.3146\n",
      "Epoch 44/200\n",
      "1356/1356 [==============================] - 1s 660us/step - loss: 0.4168 - val_loss: 0.3097\n",
      "Epoch 45/200\n",
      "1356/1356 [==============================] - 1s 650us/step - loss: 0.4189 - val_loss: 0.3123\n",
      "Epoch 46/200\n",
      "1356/1356 [==============================] - 1s 662us/step - loss: 0.4084 - val_loss: 0.3091\n",
      "Epoch 47/200\n",
      "1356/1356 [==============================] - 1s 674us/step - loss: 0.4116 - val_loss: 0.3108\n",
      "Epoch 00047: early stopping\n",
      "[CV]  activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2, total=  43.7s\n",
      "[CV] activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   43.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1357 samples, validate on 425 samples\n",
      "Epoch 1/200\n",
      "1357/1357 [==============================] - 2s 1ms/step - loss: 2.2460 - val_loss: 0.8689\n",
      "Epoch 2/200\n",
      "1357/1357 [==============================] - 1s 657us/step - loss: 1.3847 - val_loss: 0.6765\n",
      "Epoch 3/200\n",
      "1357/1357 [==============================] - 1s 657us/step - loss: 1.2661 - val_loss: 0.8481\n",
      "Epoch 4/200\n",
      "1357/1357 [==============================] - 1s 658us/step - loss: 1.0766 - val_loss: 0.7157\n",
      "Epoch 5/200\n",
      "1357/1357 [==============================] - 1s 650us/step - loss: 0.9507 - val_loss: 0.9514\n",
      "Epoch 6/200\n",
      "1357/1357 [==============================] - 1s 668us/step - loss: 0.9111 - val_loss: 0.8833\n",
      "Epoch 7/200\n",
      "1357/1357 [==============================] - 1s 666us/step - loss: 0.8436 - val_loss: 0.7549\n",
      "Epoch 8/200\n",
      "1357/1357 [==============================] - 1s 684us/step - loss: 0.8118 - val_loss: 0.7240\n",
      "Epoch 9/200\n",
      "1357/1357 [==============================] - 1s 652us/step - loss: 0.7034 - val_loss: 0.7206\n",
      "Epoch 10/200\n",
      "1357/1357 [==============================] - 1s 655us/step - loss: 0.6835 - val_loss: 0.7293\n",
      "Epoch 11/200\n",
      "1357/1357 [==============================] - 1s 664us/step - loss: 0.6448 - val_loss: 0.6956\n",
      "Epoch 12/200\n",
      "1357/1357 [==============================] - 1s 675us/step - loss: 0.6041 - val_loss: 0.4931\n",
      "Epoch 13/200\n",
      "1357/1357 [==============================] - 1s 682us/step - loss: 0.5700 - val_loss: 0.5227\n",
      "Epoch 14/200\n",
      "1357/1357 [==============================] - 1s 684us/step - loss: 0.5481 - val_loss: 0.5240\n",
      "Epoch 15/200\n",
      "1357/1357 [==============================] - 1s 705us/step - loss: 0.5519 - val_loss: 0.4285\n",
      "Epoch 16/200\n",
      "1357/1357 [==============================] - 1s 708us/step - loss: 0.5382 - val_loss: 0.4383\n",
      "Epoch 17/200\n",
      "1357/1357 [==============================] - 1s 677us/step - loss: 0.5120 - val_loss: 0.4380\n",
      "Epoch 18/200\n",
      "1357/1357 [==============================] - 1s 664us/step - loss: 0.5010 - val_loss: 0.3576\n",
      "Epoch 19/200\n",
      "1357/1357 [==============================] - 1s 625us/step - loss: 0.4797 - val_loss: 0.3407\n",
      "Epoch 20/200\n",
      "1357/1357 [==============================] - 1s 627us/step - loss: 0.4780 - val_loss: 0.3586\n",
      "Epoch 21/200\n",
      "1357/1357 [==============================] - 1s 631us/step - loss: 0.4754 - val_loss: 0.3448\n",
      "Epoch 22/200\n",
      "1357/1357 [==============================] - 1s 634us/step - loss: 0.4931 - val_loss: 0.3857\n",
      "Epoch 23/200\n",
      "1357/1357 [==============================] - 1s 624us/step - loss: 0.4710 - val_loss: 0.3269\n",
      "Epoch 24/200\n",
      "1357/1357 [==============================] - 1s 629us/step - loss: 0.4613 - val_loss: 0.3380\n",
      "Epoch 25/200\n",
      "1357/1357 [==============================] - 1s 565us/step - loss: 0.4667 - val_loss: 0.3254\n",
      "Epoch 26/200\n",
      "1357/1357 [==============================] - 1s 602us/step - loss: 0.4386 - val_loss: 0.3119\n",
      "Epoch 27/200\n",
      "1357/1357 [==============================] - 1s 560us/step - loss: 0.4425 - val_loss: 0.3116\n",
      "Epoch 28/200\n",
      "1357/1357 [==============================] - 1s 580us/step - loss: 0.4322 - val_loss: 0.3241\n",
      "Epoch 29/200\n",
      "1357/1357 [==============================] - 1s 620us/step - loss: 0.4615 - val_loss: 0.3258\n",
      "Epoch 30/200\n",
      "1357/1357 [==============================] - 1s 626us/step - loss: 0.4325 - val_loss: 0.3120\n",
      "Epoch 31/200\n",
      "1357/1357 [==============================] - 1s 636us/step - loss: 0.4562 - val_loss: 0.3110\n",
      "Epoch 32/200\n",
      "1357/1357 [==============================] - 1s 628us/step - loss: 0.4168 - val_loss: 0.3158\n",
      "Epoch 33/200\n",
      "1357/1357 [==============================] - 1s 636us/step - loss: 0.4194 - val_loss: 0.3088\n",
      "Epoch 34/200\n",
      "1357/1357 [==============================] - 1s 632us/step - loss: 0.4564 - val_loss: 0.3076\n",
      "Epoch 35/200\n",
      "1357/1357 [==============================] - 1s 630us/step - loss: 0.4587 - val_loss: 0.3077\n",
      "Epoch 36/200\n",
      "1357/1357 [==============================] - 1s 628us/step - loss: 0.4165 - val_loss: 0.3138\n",
      "Epoch 37/200\n",
      "1357/1357 [==============================] - 1s 626us/step - loss: 0.4316 - val_loss: 0.3077\n",
      "Epoch 38/200\n",
      "1357/1357 [==============================] - 1s 629us/step - loss: 0.4225 - val_loss: 0.3103\n",
      "Epoch 39/200\n",
      "1357/1357 [==============================] - 1s 630us/step - loss: 0.4350 - val_loss: 0.3127\n",
      "Epoch 40/200\n",
      "1357/1357 [==============================] - 1s 624us/step - loss: 0.4239 - val_loss: 0.3087\n",
      "Epoch 41/200\n",
      "1357/1357 [==============================] - 1s 631us/step - loss: 0.4166 - val_loss: 0.3106\n",
      "Epoch 42/200\n",
      "1357/1357 [==============================] - 1s 633us/step - loss: 0.4202 - val_loss: 0.3081\n",
      "Epoch 43/200\n",
      "1357/1357 [==============================] - 1s 626us/step - loss: 0.4288 - val_loss: 0.3076\n",
      "Epoch 44/200\n",
      "1357/1357 [==============================] - 1s 629us/step - loss: 0.4315 - val_loss: 0.3076\n",
      "Epoch 45/200\n",
      "1357/1357 [==============================] - 1s 633us/step - loss: 0.4276 - val_loss: 0.3101\n",
      "Epoch 46/200\n",
      "1357/1357 [==============================] - 1s 631us/step - loss: 0.4356 - val_loss: 0.3202\n",
      "Epoch 47/200\n",
      "1357/1357 [==============================] - 1s 633us/step - loss: 0.4134 - val_loss: 0.3076\n",
      "Epoch 48/200\n",
      "1357/1357 [==============================] - 1s 716us/step - loss: 0.4214 - val_loss: 0.3078\n",
      "Epoch 49/200\n",
      "1357/1357 [==============================] - 1s 659us/step - loss: 0.4147 - val_loss: 0.3120\n",
      "Epoch 50/200\n",
      "1357/1357 [==============================] - 1s 630us/step - loss: 0.4134 - val_loss: 0.3104\n",
      "Epoch 51/200\n",
      "1357/1357 [==============================] - 1s 639us/step - loss: 0.4239 - val_loss: 0.3134\n",
      "Epoch 52/200\n",
      "1357/1357 [==============================] - 1s 626us/step - loss: 0.4045 - val_loss: 0.3076\n",
      "Epoch 53/200\n",
      "1357/1357 [==============================] - 1s 630us/step - loss: 0.4119 - val_loss: 0.3076\n",
      "Epoch 54/200\n",
      "1357/1357 [==============================] - 1s 630us/step - loss: 0.3988 - val_loss: 0.3115\n",
      "Epoch 00054: early stopping\n",
      "[CV]  activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2, total=  49.9s\n",
      "[CV] activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2 \n",
      "Train on 1357 samples, validate on 425 samples\n",
      "Epoch 1/200\n",
      "1357/1357 [==============================] - 2s 1ms/step - loss: 1.9874 - val_loss: 0.6330\n",
      "Epoch 2/200\n",
      "1357/1357 [==============================] - 1s 702us/step - loss: 1.2588 - val_loss: 0.4744\n",
      "Epoch 3/200\n",
      "1357/1357 [==============================] - 1s 707us/step - loss: 1.2094 - val_loss: 0.5469\n",
      "Epoch 4/200\n",
      "1357/1357 [==============================] - 1s 701us/step - loss: 1.1027 - val_loss: 0.5812\n",
      "Epoch 5/200\n",
      "1357/1357 [==============================] - 1s 702us/step - loss: 0.9932 - val_loss: 0.7448\n",
      "Epoch 6/200\n",
      "1357/1357 [==============================] - 1s 715us/step - loss: 0.8200 - val_loss: 0.5808\n",
      "Epoch 7/200\n",
      "1357/1357 [==============================] - 1s 744us/step - loss: 0.8005 - val_loss: 0.8576\n",
      "Epoch 8/200\n",
      "1357/1357 [==============================] - 1s 857us/step - loss: 0.7352 - val_loss: 0.7073\n",
      "Epoch 9/200\n",
      "1357/1357 [==============================] - 1s 786us/step - loss: 0.7113 - val_loss: 0.6067\n",
      "Epoch 10/200\n",
      "1357/1357 [==============================] - 1s 762us/step - loss: 0.6505 - val_loss: 0.6713\n",
      "Epoch 11/200\n",
      "1357/1357 [==============================] - 1s 789us/step - loss: 0.5822 - val_loss: 0.5398\n",
      "Epoch 12/200\n",
      "1357/1357 [==============================] - 1s 799us/step - loss: 0.5656 - val_loss: 0.5944\n",
      "Epoch 13/200\n",
      "1357/1357 [==============================] - 1s 810us/step - loss: 0.5376 - val_loss: 0.5308\n",
      "Epoch 14/200\n",
      "1357/1357 [==============================] - 1s 932us/step - loss: 0.5567 - val_loss: 0.4035\n",
      "Epoch 15/200\n",
      "1357/1357 [==============================] - 1s 781us/step - loss: 0.5284 - val_loss: 0.3670\n",
      "Epoch 16/200\n",
      "1357/1357 [==============================] - 1s 718us/step - loss: 0.5103 - val_loss: 0.3797\n",
      "Epoch 17/200\n",
      "1357/1357 [==============================] - 1s 778us/step - loss: 0.4920 - val_loss: 0.4035\n",
      "Epoch 18/200\n",
      "1357/1357 [==============================] - 1s 787us/step - loss: 0.4873 - val_loss: 0.3333\n",
      "Epoch 19/200\n",
      "1357/1357 [==============================] - 1s 821us/step - loss: 0.4596 - val_loss: 0.3398\n",
      "Epoch 20/200\n",
      "1357/1357 [==============================] - 1s 820us/step - loss: 0.4616 - val_loss: 0.3343\n",
      "Epoch 21/200\n",
      "1357/1357 [==============================] - 1s 789us/step - loss: 0.4669 - val_loss: 0.3275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/200\n",
      "1357/1357 [==============================] - 1s 804us/step - loss: 0.4472 - val_loss: 0.3193\n",
      "Epoch 23/200\n",
      "1357/1357 [==============================] - 1s 811us/step - loss: 0.4410 - val_loss: 0.3234\n",
      "Epoch 24/200\n",
      "1357/1357 [==============================] - 1s 793us/step - loss: 0.4509 - val_loss: 0.3179\n",
      "Epoch 25/200\n",
      "1357/1357 [==============================] - 1s 913us/step - loss: 0.4569 - val_loss: 0.3126\n",
      "Epoch 26/200\n",
      "1357/1357 [==============================] - 1s 802us/step - loss: 0.4531 - val_loss: 0.3265\n",
      "Epoch 27/200\n",
      "1357/1357 [==============================] - 1s 877us/step - loss: 0.4597 - val_loss: 0.3076\n",
      "Epoch 28/200\n",
      "1357/1357 [==============================] - 1s 773us/step - loss: 0.4404 - val_loss: 0.3086\n",
      "Epoch 29/200\n",
      "1357/1357 [==============================] - 1s 817us/step - loss: 0.4534 - val_loss: 0.3108\n",
      "Epoch 30/200\n",
      "1357/1357 [==============================] - 1s 954us/step - loss: 0.4491 - val_loss: 0.3080\n",
      "Epoch 31/200\n",
      "1357/1357 [==============================] - 1s 807us/step - loss: 0.4210 - val_loss: 0.3083\n",
      "Epoch 32/200\n",
      "1357/1357 [==============================] - 1s 771us/step - loss: 0.4176 - val_loss: 0.3096\n",
      "Epoch 33/200\n",
      "1357/1357 [==============================] - 1s 789us/step - loss: 0.4378 - val_loss: 0.3081\n",
      "Epoch 34/200\n",
      "1357/1357 [==============================] - 1s 749us/step - loss: 0.4444 - val_loss: 0.3098\n",
      "Epoch 35/200\n",
      "1357/1357 [==============================] - 1s 738us/step - loss: 0.4167 - val_loss: 0.3078\n",
      "Epoch 36/200\n",
      "1357/1357 [==============================] - 1s 749us/step - loss: 0.4275 - val_loss: 0.3079\n",
      "Epoch 37/200\n",
      "1357/1357 [==============================] - 1s 764us/step - loss: 0.4219 - val_loss: 0.3076\n",
      "Epoch 38/200\n",
      "1357/1357 [==============================] - 1s 764us/step - loss: 0.4111 - val_loss: 0.3077\n",
      "Epoch 39/200\n",
      "1357/1357 [==============================] - 1s 833us/step - loss: 0.4275 - val_loss: 0.3076\n",
      "Epoch 40/200\n",
      "1357/1357 [==============================] - 1s 778us/step - loss: 0.4079 - val_loss: 0.3084\n",
      "Epoch 41/200\n",
      "1357/1357 [==============================] - 1s 684us/step - loss: 0.4130 - val_loss: 0.3076\n",
      "Epoch 42/200\n",
      "1357/1357 [==============================] - 1s 658us/step - loss: 0.4079 - val_loss: 0.3120\n",
      "Epoch 43/200\n",
      "1357/1357 [==============================] - 1s 700us/step - loss: 0.4354 - val_loss: 0.3105\n",
      "Epoch 44/200\n",
      "1357/1357 [==============================] - 1s 881us/step - loss: 0.4134 - val_loss: 0.3137\n",
      "Epoch 45/200\n",
      "1357/1357 [==============================] - 1s 904us/step - loss: 0.4216 - val_loss: 0.3203\n",
      "Epoch 46/200\n",
      "1357/1357 [==============================] - 1s 780us/step - loss: 0.4241 - val_loss: 0.3099\n",
      "Epoch 47/200\n",
      "1357/1357 [==============================] - 1s 763us/step - loss: 0.4017 - val_loss: 0.3152\n",
      "Epoch 00047: early stopping\n",
      "[CV]  activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2, total=  52.9s\n",
      "[CV] activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2 \n",
      "Train on 1357 samples, validate on 425 samples\n",
      "Epoch 1/200\n",
      "1357/1357 [==============================] - 2s 1ms/step - loss: 1.9418 - val_loss: 0.5321\n",
      "Epoch 2/200\n",
      "1357/1357 [==============================] - 1s 732us/step - loss: 1.4245 - val_loss: 0.5919\n",
      "Epoch 3/200\n",
      "1357/1357 [==============================] - 1s 704us/step - loss: 1.1293 - val_loss: 0.8700\n",
      "Epoch 4/200\n",
      "1357/1357 [==============================] - 1s 896us/step - loss: 0.9751 - val_loss: 1.0120\n",
      "Epoch 5/200\n",
      "1357/1357 [==============================] - 1s 856us/step - loss: 0.9478 - val_loss: 1.0337\n",
      "Epoch 6/200\n",
      "1357/1357 [==============================] - 1s 733us/step - loss: 0.8526 - val_loss: 0.8819\n",
      "Epoch 7/200\n",
      "1357/1357 [==============================] - 1s 686us/step - loss: 0.7658 - val_loss: 0.6837\n",
      "Epoch 8/200\n",
      "1357/1357 [==============================] - 1s 702us/step - loss: 0.6897 - val_loss: 0.5574\n",
      "Epoch 9/200\n",
      "1357/1357 [==============================] - 1s 694us/step - loss: 0.6807 - val_loss: 0.6847\n",
      "Epoch 10/200\n",
      "1357/1357 [==============================] - 1s 665us/step - loss: 0.6684 - val_loss: 0.5581\n",
      "Epoch 11/200\n",
      "1357/1357 [==============================] - 1s 609us/step - loss: 0.5770 - val_loss: 0.5655\n",
      "Epoch 12/200\n",
      "1357/1357 [==============================] - 1s 558us/step - loss: 0.6228 - val_loss: 0.4687\n",
      "Epoch 13/200\n",
      "1357/1357 [==============================] - 1s 579us/step - loss: 0.5434 - val_loss: 0.4895\n",
      "Epoch 14/200\n",
      "1357/1357 [==============================] - 1s 685us/step - loss: 0.5757 - val_loss: 0.4178\n",
      "Epoch 15/200\n",
      "1357/1357 [==============================] - 1s 661us/step - loss: 0.5019 - val_loss: 0.4036\n",
      "Epoch 16/200\n",
      "1357/1357 [==============================] - 1s 658us/step - loss: 0.5388 - val_loss: 0.4293\n",
      "Epoch 17/200\n",
      "1357/1357 [==============================] - 1s 674us/step - loss: 0.5170 - val_loss: 0.3840\n",
      "Epoch 18/200\n",
      "1357/1357 [==============================] - 1s 981us/step - loss: 0.4878 - val_loss: 0.3881\n",
      "Epoch 19/200\n",
      "1357/1357 [==============================] - 1s 706us/step - loss: 0.4509 - val_loss: 0.3385\n",
      "Epoch 20/200\n",
      "1357/1357 [==============================] - 1s 602us/step - loss: 0.4634 - val_loss: 0.3502\n",
      "Epoch 21/200\n",
      "1357/1357 [==============================] - 1s 647us/step - loss: 0.4461 - val_loss: 0.3480\n",
      "Epoch 22/200\n",
      "1357/1357 [==============================] - 1s 660us/step - loss: 0.4586 - val_loss: 0.3460\n",
      "Epoch 23/200\n",
      "1357/1357 [==============================] - 1s 658us/step - loss: 0.4503 - val_loss: 0.3519\n",
      "Epoch 24/200\n",
      "1357/1357 [==============================] - 1s 615us/step - loss: 0.4516 - val_loss: 0.3149\n",
      "Epoch 25/200\n",
      "1357/1357 [==============================] - 1s 623us/step - loss: 0.4384 - val_loss: 0.3202\n",
      "Epoch 26/200\n",
      "1357/1357 [==============================] - 1s 600us/step - loss: 0.4642 - val_loss: 0.3636\n",
      "Epoch 27/200\n",
      "1357/1357 [==============================] - 1s 655us/step - loss: 0.4633 - val_loss: 0.3124\n",
      "Epoch 28/200\n",
      "1357/1357 [==============================] - 1s 618us/step - loss: 0.4353 - val_loss: 0.3281\n",
      "Epoch 29/200\n",
      "1357/1357 [==============================] - 1s 605us/step - loss: 0.4430 - val_loss: 0.3154\n",
      "Epoch 30/200\n",
      "1357/1357 [==============================] - 1s 620us/step - loss: 0.4378 - val_loss: 0.3197\n",
      "Epoch 31/200\n",
      "1357/1357 [==============================] - 1s 612us/step - loss: 0.4298 - val_loss: 0.3197\n",
      "Epoch 32/200\n",
      "1357/1357 [==============================] - 1s 609us/step - loss: 0.4077 - val_loss: 0.3202\n",
      "Epoch 33/200\n",
      "1357/1357 [==============================] - 1s 613us/step - loss: 0.4230 - val_loss: 0.3080\n",
      "Epoch 34/200\n",
      "1357/1357 [==============================] - 1s 635us/step - loss: 0.4211 - val_loss: 0.3099\n",
      "Epoch 35/200\n",
      "1357/1357 [==============================] - 1s 842us/step - loss: 0.4321 - val_loss: 0.3091\n",
      "Epoch 36/200\n",
      "1357/1357 [==============================] - 1s 628us/step - loss: 0.4335 - val_loss: 0.3101\n",
      "Epoch 37/200\n",
      "1357/1357 [==============================] - 1s 617us/step - loss: 0.4127 - val_loss: 0.3076\n",
      "Epoch 38/200\n",
      "1357/1357 [==============================] - 1s 709us/step - loss: 0.4061 - val_loss: 0.3078\n",
      "Epoch 39/200\n",
      "1357/1357 [==============================] - 1s 677us/step - loss: 0.4116 - val_loss: 0.3083\n",
      "Epoch 40/200\n",
      "1357/1357 [==============================] - 1s 663us/step - loss: 0.4198 - val_loss: 0.3110\n",
      "Epoch 41/200\n",
      "1357/1357 [==============================] - 1s 658us/step - loss: 0.4269 - val_loss: 0.3149\n",
      "Epoch 42/200\n",
      "1357/1357 [==============================] - 1s 666us/step - loss: 0.4307 - val_loss: 0.3080\n",
      "Epoch 43/200\n",
      "1357/1357 [==============================] - 1s 657us/step - loss: 0.4168 - val_loss: 0.3079\n",
      "Epoch 44/200\n",
      "1357/1357 [==============================] - 1s 633us/step - loss: 0.4081 - val_loss: 0.3142\n",
      "Epoch 45/200\n",
      "1357/1357 [==============================] - 1s 690us/step - loss: 0.4097 - val_loss: 0.3138\n",
      "Epoch 46/200\n",
      "1357/1357 [==============================] - 1s 671us/step - loss: 0.3865 - val_loss: 0.3161\n",
      "Epoch 47/200\n",
      "1357/1357 [==============================] - 1s 608us/step - loss: 0.4145 - val_loss: 0.3091\n",
      "Epoch 48/200\n",
      "1357/1357 [==============================] - 1s 626us/step - loss: 0.4237 - val_loss: 0.3100\n",
      "Epoch 49/200\n",
      "1357/1357 [==============================] - 1s 753us/step - loss: 0.4117 - val_loss: 0.3078\n",
      "Epoch 50/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1357/1357 [==============================] - 1s 803us/step - loss: 0.3961 - val_loss: 0.3076\n",
      "Epoch 51/200\n",
      "1357/1357 [==============================] - 1s 706us/step - loss: 0.3995 - val_loss: 0.3145\n",
      "Epoch 52/200\n",
      "1357/1357 [==============================] - 1s 678us/step - loss: 0.4121 - val_loss: 0.3080\n",
      "Epoch 53/200\n",
      "1357/1357 [==============================] - 1s 628us/step - loss: 0.4045 - val_loss: 0.3154\n",
      "Epoch 00053: early stopping\n",
      "[CV]  activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2, total=  51.9s\n",
      "[CV] activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2 \n",
      "Train on 1357 samples, validate on 425 samples\n",
      "Epoch 1/200\n",
      "1357/1357 [==============================] - 2s 1ms/step - loss: 2.0450 - val_loss: 0.8345\n",
      "Epoch 2/200\n",
      "1357/1357 [==============================] - 1s 654us/step - loss: 1.3535 - val_loss: 0.6536\n",
      "Epoch 3/200\n",
      "1357/1357 [==============================] - 1s 634us/step - loss: 1.1879 - val_loss: 0.9440\n",
      "Epoch 4/200\n",
      "1357/1357 [==============================] - 1s 642us/step - loss: 1.0438 - val_loss: 0.6805\n",
      "Epoch 5/200\n",
      "1357/1357 [==============================] - 1s 671us/step - loss: 0.9833 - val_loss: 0.7556\n",
      "Epoch 6/200\n",
      "1357/1357 [==============================] - 1s 701us/step - loss: 0.8581 - val_loss: 0.7357\n",
      "Epoch 7/200\n",
      "1357/1357 [==============================] - 1s 628us/step - loss: 0.7406 - val_loss: 0.7627\n",
      "Epoch 8/200\n",
      "1357/1357 [==============================] - 1s 596us/step - loss: 0.7406 - val_loss: 0.6750\n",
      "Epoch 9/200\n",
      "1357/1357 [==============================] - 1s 659us/step - loss: 0.6572 - val_loss: 0.6222\n",
      "Epoch 10/200\n",
      "1357/1357 [==============================] - 1s 643us/step - loss: 0.6401 - val_loss: 0.6977\n",
      "Epoch 11/200\n",
      "1357/1357 [==============================] - 1s 1ms/step - loss: 0.6270 - val_loss: 0.4944\n",
      "Epoch 12/200\n",
      "1357/1357 [==============================] - 1s 715us/step - loss: 0.5783 - val_loss: 0.4939\n",
      "Epoch 13/200\n",
      "1357/1357 [==============================] - 1s 672us/step - loss: 0.5504 - val_loss: 0.4826\n",
      "Epoch 14/200\n",
      "1357/1357 [==============================] - 1s 696us/step - loss: 0.5302 - val_loss: 0.4409\n",
      "Epoch 15/200\n",
      "1357/1357 [==============================] - 1s 704us/step - loss: 0.5055 - val_loss: 0.4057\n",
      "Epoch 16/200\n",
      "1357/1357 [==============================] - 1s 620us/step - loss: 0.4608 - val_loss: 0.4142\n",
      "Epoch 17/200\n",
      "1357/1357 [==============================] - 1s 561us/step - loss: 0.4922 - val_loss: 0.3534\n",
      "Epoch 18/200\n",
      "1357/1357 [==============================] - 1s 668us/step - loss: 0.4990 - val_loss: 0.3539\n",
      "Epoch 19/200\n",
      "1357/1357 [==============================] - 1s 655us/step - loss: 0.4613 - val_loss: 0.3446\n",
      "Epoch 20/200\n",
      "1357/1357 [==============================] - 1s 679us/step - loss: 0.4793 - val_loss: 0.3225\n",
      "Epoch 21/200\n",
      "1357/1357 [==============================] - 1s 681us/step - loss: 0.4698 - val_loss: 0.3576\n",
      "Epoch 22/200\n",
      "1357/1357 [==============================] - 1s 654us/step - loss: 0.4267 - val_loss: 0.3717\n",
      "Epoch 23/200\n",
      "1357/1357 [==============================] - 1s 671us/step - loss: 0.4532 - val_loss: 0.3154\n",
      "Epoch 24/200\n",
      "1357/1357 [==============================] - 1s 639us/step - loss: 0.4443 - val_loss: 0.3122\n",
      "Epoch 25/200\n",
      "1357/1357 [==============================] - 1s 664us/step - loss: 0.4246 - val_loss: 0.3410\n",
      "Epoch 26/200\n",
      "1357/1357 [==============================] - 1s 672us/step - loss: 0.4614 - val_loss: 0.3111\n",
      "Epoch 27/200\n",
      "1357/1357 [==============================] - 1s 664us/step - loss: 0.4680 - val_loss: 0.3172\n",
      "Epoch 28/200\n",
      "1357/1357 [==============================] - 1s 648us/step - loss: 0.4498 - val_loss: 0.3099\n",
      "Epoch 29/200\n",
      "1357/1357 [==============================] - 1s 659us/step - loss: 0.4266 - val_loss: 0.3075\n",
      "Epoch 30/200\n",
      "1357/1357 [==============================] - 1s 671us/step - loss: 0.4325 - val_loss: 0.3190\n",
      "Epoch 31/200\n",
      "1357/1357 [==============================] - 1s 708us/step - loss: 0.4290 - val_loss: 0.3243\n",
      "Epoch 32/200\n",
      "1357/1357 [==============================] - 1s 692us/step - loss: 0.4104 - val_loss: 0.3127\n",
      "Epoch 33/200\n",
      "1357/1357 [==============================] - 1s 663us/step - loss: 0.4326 - val_loss: 0.3087\n",
      "Epoch 34/200\n",
      "1357/1357 [==============================] - 1s 670us/step - loss: 0.4279 - val_loss: 0.3075\n",
      "Epoch 35/200\n",
      "1357/1357 [==============================] - 1s 662us/step - loss: 0.4544 - val_loss: 0.3155\n",
      "Epoch 36/200\n",
      "1357/1357 [==============================] - 1s 663us/step - loss: 0.4108 - val_loss: 0.3088\n",
      "Epoch 37/200\n",
      "1357/1357 [==============================] - 1s 721us/step - loss: 0.4266 - val_loss: 0.3099\n",
      "Epoch 38/200\n",
      "1357/1357 [==============================] - 1s 655us/step - loss: 0.4309 - val_loss: 0.3087\n",
      "Epoch 39/200\n",
      "1357/1357 [==============================] - 1s 648us/step - loss: 0.4053 - val_loss: 0.3147\n",
      "Epoch 40/200\n",
      "1357/1357 [==============================] - 1s 711us/step - loss: 0.4129 - val_loss: 0.3188\n",
      "Epoch 41/200\n",
      "1357/1357 [==============================] - 1s 674us/step - loss: 0.4079 - val_loss: 0.3078\n",
      "Epoch 42/200\n",
      "1357/1357 [==============================] - 1s 671us/step - loss: 0.4191 - val_loss: 0.3097\n",
      "Epoch 43/200\n",
      "1357/1357 [==============================] - 1s 667us/step - loss: 0.4154 - val_loss: 0.3097\n",
      "Epoch 44/200\n",
      "1357/1357 [==============================] - 1s 679us/step - loss: 0.4144 - val_loss: 0.3076\n",
      "Epoch 45/200\n",
      "1357/1357 [==============================] - 1s 677us/step - loss: 0.4032 - val_loss: 0.3083\n",
      "Epoch 46/200\n",
      "1357/1357 [==============================] - 1s 672us/step - loss: 0.3975 - val_loss: 0.3083\n",
      "Epoch 47/200\n",
      "1357/1357 [==============================] - 1s 673us/step - loss: 0.3928 - val_loss: 0.3082\n",
      "Epoch 48/200\n",
      "1357/1357 [==============================] - 1s 666us/step - loss: 0.4098 - val_loss: 0.3107\n",
      "Epoch 49/200\n",
      "1357/1357 [==============================] - 1s 704us/step - loss: 0.4143 - val_loss: 0.3075\n",
      "Epoch 00049: early stopping\n",
      "[CV]  activation_function=tanh, hidden_layer_size=64, num_hidden_layers=2, total=  48.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1696 samples, validate on 425 samples\n",
      "Epoch 1/200\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 1.7650 - val_loss: 0.5314\n",
      "Epoch 2/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 1.3144 - val_loss: 0.6059\n",
      "Epoch 3/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 1.0397 - val_loss: 0.8929\n",
      "Epoch 4/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.9217 - val_loss: 0.7817\n",
      "Epoch 5/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.8215 - val_loss: 0.8265\n",
      "Epoch 6/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.7355 - val_loss: 0.6292\n",
      "Epoch 7/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.6849 - val_loss: 0.7167\n",
      "Epoch 8/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.6497 - val_loss: 0.6692\n",
      "Epoch 9/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.5960 - val_loss: 0.6133\n",
      "Epoch 10/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.5975 - val_loss: 0.4770\n",
      "Epoch 11/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.5276 - val_loss: 0.5429\n",
      "Epoch 12/200\n",
      "1696/1696 [==============================] - 2s 928us/step - loss: 0.4870 - val_loss: 0.4168\n",
      "Epoch 13/200\n",
      "1696/1696 [==============================] - 2s 974us/step - loss: 0.5138 - val_loss: 0.4641\n",
      "Epoch 14/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4706 - val_loss: 0.4337\n",
      "Epoch 15/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.5009 - val_loss: 0.3196\n",
      "Epoch 16/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4757 - val_loss: 0.3611\n",
      "Epoch 17/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4661 - val_loss: 0.3410\n",
      "Epoch 18/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4556 - val_loss: 0.3505\n",
      "Epoch 19/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4622 - val_loss: 0.3203\n",
      "Epoch 20/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4532 - val_loss: 0.3232\n",
      "Epoch 21/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4374 - val_loss: 0.3080\n",
      "Epoch 22/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4329 - val_loss: 0.3076\n",
      "Epoch 23/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4425 - val_loss: 0.3080\n",
      "Epoch 24/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4285 - val_loss: 0.3214\n",
      "Epoch 25/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4405 - val_loss: 0.3077\n",
      "Epoch 26/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4199 - val_loss: 0.3077\n",
      "Epoch 27/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4054 - val_loss: 0.3079\n",
      "Epoch 28/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4247 - val_loss: 0.3106\n",
      "Epoch 29/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4228 - val_loss: 0.3082\n",
      "Epoch 30/200\n",
      "1696/1696 [==============================] - 2s 996us/step - loss: 0.4314 - val_loss: 0.3097\n",
      "Epoch 31/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4059 - val_loss: 0.3080\n",
      "Epoch 32/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4276 - val_loss: 0.3077\n",
      "Epoch 33/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4228 - val_loss: 0.3086\n",
      "Epoch 34/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4106 - val_loss: 0.3100\n",
      "Epoch 35/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4057 - val_loss: 0.3078\n",
      "Epoch 36/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4053 - val_loss: 0.3082\n",
      "Epoch 37/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4058 - val_loss: 0.3115\n",
      "Epoch 38/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4117 - val_loss: 0.3154\n",
      "Epoch 39/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4111 - val_loss: 0.3079\n",
      "Epoch 40/200\n",
      "1696/1696 [==============================] - 2s 1ms/step - loss: 0.4002 - val_loss: 0.3078\n",
      "Epoch 41/200\n",
      "1696/1696 [==============================] - 2s 956us/step - loss: 0.3850 - val_loss: 0.3079\n",
      "Epoch 00041: early stopping\n",
      "Out-of-sample MSE:  0.30794313042794835\n",
      "CPU times: user 5min 7s, sys: 22.5 s, total: 5min 30s\n",
      "Wall time: 5min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_info = {}\n",
    "for i, p in enumerate(final_models):\n",
    "    history = History() # History callback\n",
    "    model_name = 'model_'+str(i+1)\n",
    "    classifier = KerasRegressor(make_model, batch_size=32, epochs=200)\n",
    "    grid = GridSearchCV(classifier,\n",
    "                             param_grid=p,\n",
    "                             scoring='neg_mean_squared_error', #sklearn optimizing by maximizing negative MSE\n",
    "                             n_jobs=1,\n",
    "                             verbose=2,\n",
    "                             cv=KFold(n_splits=5, shuffle=True, random_state=42),# Number of folds for CV\n",
    "                             return_train_score=True\n",
    "                       )\n",
    "\n",
    "    grid.fit(np.array(X_train), np.array(y_train),\n",
    "            **{'callbacks': [EarlyStopping(monitor='val_loss', \n",
    "                                                                     min_delta=0.001, \n",
    "                                                                     patience=20,\n",
    "                                                                     mode='min',\n",
    "                                                                     verbose=2),\n",
    "                                                      CSVLogger('../../output/logs/'+model_name+'.csv', separator=',', append=True),\n",
    "                                                      history], # Added a history callback\n",
    "                                        'validation_data': (np.array(X_test), np.array(y_test))\n",
    "                                        })\n",
    "    y_hats = grid.predict(np.array(X_test))\n",
    "    print(\"Out-of-sample MSE: \", mean_squared_error(y_test, y_hats))\n",
    "    y_hats_full = grid.predict(np.array(X))\n",
    "    model_info[model_name] = {'grid_obj': grid,\n",
    "                              'keras_model': grid.best_estimator_.model,\n",
    "                              'preds': y_hats_full,\n",
    "                              'history': history.history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_1': {'grid_obj': GridSearchCV(cv=KFold(n_splits=5, random_state=42, shuffle=True),\n",
       "         error_score='raise-deprecating',\n",
       "         estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x15b6a6e48>,\n",
       "         fit_params=None, iid='warn', n_jobs=1,\n",
       "         param_grid={'num_hidden_layers': [2], 'hidden_layer_size': [64], 'activation_function': ['tanh']},\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "         scoring='neg_mean_squared_error', verbose=2),\n",
       "  'keras_model': <keras.engine.sequential.Sequential at 0x15f469ef0>,\n",
       "  'preds': array([2.8838518, 2.883857 , 2.883857 , ..., 2.883857 , 2.883857 ,\n",
       "         2.883857 ], dtype=float32),\n",
       "  'history': {'val_loss': [0.5313826993633719,\n",
       "    0.6059455791641685,\n",
       "    0.8929229417969199,\n",
       "    0.7817382983600392,\n",
       "    0.8265372492285336,\n",
       "    0.6292406084958245,\n",
       "    0.7166805403372821,\n",
       "    0.6691630902009852,\n",
       "    0.6132755932387184,\n",
       "    0.4769882801701041,\n",
       "    0.5428759325953091,\n",
       "    0.41675986093633316,\n",
       "    0.46410010814666747,\n",
       "    0.43370567013235656,\n",
       "    0.3196144571023829,\n",
       "    0.3611044413201949,\n",
       "    0.3409587893766515,\n",
       "    0.3505312465920168,\n",
       "    0.32033277672879834,\n",
       "    0.3232217784488902,\n",
       "    0.30795030614909,\n",
       "    0.30759250837213853,\n",
       "    0.30796066732967603,\n",
       "    0.32142231786952297,\n",
       "    0.3076864962016835,\n",
       "    0.30765317068380466,\n",
       "    0.3078861929388607,\n",
       "    0.31058696438284483,\n",
       "    0.3082242532337413,\n",
       "    0.3096889320541831,\n",
       "    0.3079757074047537,\n",
       "    0.3076632254263934,\n",
       "    0.30855987780234395,\n",
       "    0.30999597535413853,\n",
       "    0.30779371717396903,\n",
       "    0.30815928101539614,\n",
       "    0.3115428766082315,\n",
       "    0.3153724673215081,\n",
       "    0.3079361110575059,\n",
       "    0.3077724086537081,\n",
       "    0.30794313318589156],\n",
       "   'loss': [1.765018807267243,\n",
       "    1.31442735555037,\n",
       "    1.039722276183794,\n",
       "    0.9217441919839607,\n",
       "    0.8215248022439345,\n",
       "    0.7354965035645467,\n",
       "    0.6849088320192301,\n",
       "    0.6496509468218066,\n",
       "    0.5959832010404119,\n",
       "    0.5974640700052369,\n",
       "    0.5275797365971331,\n",
       "    0.4869726047200977,\n",
       "    0.5137916491279062,\n",
       "    0.4705871804705206,\n",
       "    0.5009015922276479,\n",
       "    0.47573887233464224,\n",
       "    0.46612558556052874,\n",
       "    0.4555610530781296,\n",
       "    0.4622196228999012,\n",
       "    0.45316988061059194,\n",
       "    0.437405434700678,\n",
       "    0.4328542843742191,\n",
       "    0.4424625986589576,\n",
       "    0.428520013701241,\n",
       "    0.4404917314367474,\n",
       "    0.41985739177128056,\n",
       "    0.4053914060570159,\n",
       "    0.4246861515742428,\n",
       "    0.42278692379312693,\n",
       "    0.4314056123202702,\n",
       "    0.4059409926522453,\n",
       "    0.42758557172316425,\n",
       "    0.4227633793961327,\n",
       "    0.41063162487632826,\n",
       "    0.40572704290443995,\n",
       "    0.4053195037369458,\n",
       "    0.40577062402131425,\n",
       "    0.4117286261522545,\n",
       "    0.4110808105401273,\n",
       "    0.40019955247078304,\n",
       "    0.3850132870786595]}}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = {k:v['history'] for k,v in model_info.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(histories, open('../../output/model_histories.p','wb')) # Storing for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess how the different models performed we can plot their performance on the training and test set at each epoch in the learning process. Since the first couple of epochs generally have a very high loss (thus increasing the size of the y-axis and obscuring the future epochs) I present the learn from the results from the second epoch onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_hidden_layers': [2], 'hidden_layer_size': [64], 'activation_function': ['tanh']}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, k in enumerate(list(model_info.keys())): \n",
    "    print(final_models[i])\n",
    "    plt.plot(model_info[k]['history']['loss'][2:])\n",
    "    plt.plot(model_info[k]['history']['val_loss'][2:])\n",
    "    plt.title('Mean squared error over training')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and third models both show evidence of overfitting, as training performance continued to improve as validation performance plateaued (as seen when the blue line goes below the orange line). This suggests that these models will perform worse on the out-of-sample data.\n",
    "\n",
    "Now I store and upload the predictions of each model to obtain the results on the true held out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, k in enumerate(list(model_info.keys())):\n",
    "    predictions['gpa'] = model_info[k]['preds']\n",
    "    name = '../../output/final_predictions_model_'+str(i)+'.csv'\n",
    "    predictions.to_csv(name)\n",
    "    # csvs were then uploaded to the challenge website: https://codalab.fragilefamilieschallenge.org/#participate-submit_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to compare to the predicted values for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30794313042794835\n"
     ]
    }
   ],
   "source": [
    "# Printing performance on validation set\n",
    "for k,v in model_info.items():\n",
    "    print(mean_squared_error(y_test, v['grid_obj'].predict(np.array(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all five model objects\n",
    "for k,v in model_info.items():\n",
    "    v['keras_model'].save('../../output/models/'+k+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
